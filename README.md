# deep_radiologist
Uses a convolutional neural network to do heatmap regression.


## Setup

1. Clone this repository from github

2. Make a python virtual environment in the root directory (if not already present).
```bash
python3.9 -m venv venv
```
See [here](https://towardsdatascience.com/getting-started-with-python-virtual-environments-252a6bd2240) 
for more information on virtual environments.

2. Activate the python virtual environment.
(On linux/macos)
```bash
source venv/bin/activate
```

(On Windows)
```
venv\Scripts\activate
```

3. Install dependencies
```bash
pip install -r requirements.txt
```

When done, you can close the terminal, or deactivate the python virtual environment with
```bash
deactivate
```

## Quick start (for linux)

1. Activate the python virtual environment.
```bash
source venv/bin/activate
```


### Prepare your dataset

2. Add the files you want to use for images and labels to a data source specifier csv file with 3 columns: 
- `image_file_path` (path to the dicom or nifti file used to annotate with mctv)
- `label_file_path` (matlab files with corneas and rhabdoms generated by mctv)
- `split` (containing 'train' or 'test' to say how the data should be split)
		This should be generated randomly, or rerun multiple times as part of a
		k-fold cross validation process.
Example:
		data_source_specifiers/fiddler.csv

This file should be placed in the ./data_source_specifiers directory.

3. Generate the dataset (if not already found in the `./dataset/` directory) using the data source specifier csv file from step 2.
```bash
python generate_dataset.py ./data_sources/fiddler.csv -v 10 20 25
```

4. Now, you need to make a decision as to what resample size you want to use for your dataset. The `generate_dataset.py` command used above will have
generated three sizes for you: average of 10 voxels between labels, 20 voxels between labels and 25 voxels between labels (you can edit the previous command
if you want to generate others by editing the aguments after the `-v` flag).

You should open these images with a 3d volume viewer (e.g. Dragonfly) and see what resampled resolution is suitable for detecting your features of interest.

One thing to consider in this decision is that a image/volume with too few voxels may not provide enough information for the model to detect the
feature, whereas a image with too many voxels may have so much information that it cannot be loaded into your computer's memory, or require an
unreasonably large training time or model size. If you face memory usage issues during training or inference, consider reducing the voxel spacing used.

For the given paper, I used the smallest voxel spacing that I could use to make out the features of interest (10 voxels for fiddler crab corneas and rhabdoms and 20 voxels
for paraphronima corneas and rhabdoms).

To make a decision, use paths to the folders ending in the specified number of voxels in the following step (step 5).

5. Add these paths from step 4 to the `train_images_dir`, `train_labels_dir`, `test_images_dir` and `test_labels_dir` in your config file.
You should create a new one of these for each of your tasks. You can base these on one of the examples: e.g. the `configs/fiddlercrab_cornea_config.yaml` file.

6. Change the `label_suffix` parameter in your config file to the name of the label that you want to detect. E.g. 'corneas' or 'rhabdoms'.

7. Remove empty labels from this dataset, specifying the path to your config file as an argument
```bash
python remove_empty_training_data.py configs/fiddlercrab_cornea_config.yaml
```


### Train

8. Start training, specifying the path to your config file as an argument
```bash
python main.py train configs/fiddlercrab_cornea_config.yaml
```

(To view live training progress charts, open a new terminal in this directory and start up tensorboard)
```bash
source venv/bin/activate
tensorboard --logdir lightning_logs
```

Once you are happy with a model's performance, copy and paste its folder (in the lightning_logs directory
(e.g. version_5) to the the `zoo/` folder. This is so you can keep track of your models in a zoo, and easily
use them for running inference.

Its advisable that you change the `debug_plots` variable in your `config.yaml` file to `True` before running `python main.py train`,
so that you can verify whether your parameters to locate peaks are suitable. Pay particular attention to the `peak_min_distance` and
`peak_min_val` variables. After you've verified everything looks ok, change `debug_plots` back to `False`.


### Optimise hyperparameters

9. (OPTIONAL) If you are unhappy with your models performance, it may be that the hyperparameters you are using, are not well suited
to your problem.

You can either:

- manually adjust these in the `config.yaml` file, and rerun training step 8 above.

- or, automatically optimise these parameters with a hyperparameter search:
```
python main.py tune
```
I have implemented what the search method in the `main.py` file and what hyperparameters to search for
in the `actions.py` file, under the `objective()` function. You can edit this function to change
what values you want to search for. See
https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize for more info.

If your computer can run multiple instances of the hyperparameter search/tuning process, open more terminals and type
```
source venv/bin/activate
python main.py tune
```
to run the job in a parallised way and make the search process faster.

(To view live training progress charts, open a new terminal in this directory and start up tensorboard)
```bash
source venv/bin/activate
tensorboard --logdir lightning_logs
```

Once you have found suitable hyperparameters, and as you find better hyperparameters, they will be printed to the console
and also saved with their performance in a sqlite database. You should take these best values and use them to update your `config.yaml` file.


### Inference

10. Once you have a trained model, you can use it to make predictions, otherwise known as inference.

To do so, you will first have to resample the test volume to have approximately the same number of voxels between the labels
you are looking to detect as the images used for training. If you ran step 3, these will have already been generated for you
in a folder called something like: `data/fiddler/whole/test_images/`

Then run the following command, specifying the path to this volume and the path to your trained model
```
python main.py inference ./dataset/fiddler/whole/test_images/dampieri_male_16-image.nii.gz ./zoo/fiddler_crab_corneas/version_4/
```




## File overview
```
.
├── data_info.csv  - details where annotated MATLAB files and corresponding volumes are stored
├── dataset/  - the local copy of the dataset used for analysis. This should likely be symlinked to a data drive
├── generate_dataset.py  - a script to generate your dataset
├── landmarks.npy  - a file created for histogram standardisation by torchio_data_transform.ipynb
├── lightning_logs/  - where model trial files are stored
├── main.py  - the main script to run for training, testing and inference
├── deep_radiologist/  - helper classes and functions used specifically for this project
├── old_development_stuff/  - old files used during prototyping
├── output.png  - an example output of the model
├── README.md
├── remove_empty_training_data.py  - an experimental option to remove data with no labels
├── requirements.txt
├── scripts/  - helpful bash scripts for setup
└── torchio_data_transform.ipynb  - a file used to explore transformations of the data and generate the landmarks.npy file for histogram standardisation
```
