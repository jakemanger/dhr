# DeepRadiologist
Uses a convolutional neural network to do heatmap regression.


## Setup

1. Clone this repository from github

2. Make a python virtual environment in the root directory (if not already present).
```bash
python3.9 -m venv venv
```
See [here](https://towardsdatascience.com/getting-started-with-python-virtual-environments-252a6bd2240) 
for more information on virtual environments.

2. Activate the python virtual environment.
(On linux/macos)
```bash
source venv/bin/activate
```

(On Windows)
```
venv\Scripts\activate
```

3. Install dependencies
```bash
pip install -r requirements.txt
```

When done, you can close the terminal, or deactivate the python virtual environment with
```bash
deactivate
```

## Quick start (for linux)

1. Activate the python virtual environment.
```bash
source venv/bin/activate
```


### Prepare your dataset

2. Add the files you want to use for images and labels to a data source specifier csv file with 3 columns: 
- `image_file_path` (path to the dicom or nifti file used to annotate with mctv)
- `label_file_path` (matlab files with corneas and rhabdoms generated by mctv)
- `split` (containing 'train' or 'test' to say how the data should be split)
		This should be generated randomly, or rerun multiple times as part of a
		k-fold cross validation process.
Example:
		data_source_specifiers/fiddlercrab_corneas.csv

This file should be placed in the ./data_source_specifiers directory.


3. Generate the dataset (if not already found in the `./dataset/` directory) using the data source specifier csv file from step 2.
```bash
python generate_dataset.py ./data_source_specifiers/fiddlercrab_corneas.csv -v 10 15 20 25 30
```
*The `generate_dataset.py` command used above will have
generated five sizes for you: average of 10, 15, 20, 25 and 30 voxels between labels (you can edit the previous command
if you want to generate others by changing the aguments after the `-v` flag).*


4. Create a new config file in the configs directory. You should create a new one of these for each of your tasks. You can base these on one of the examples: e.g. the `configs/fiddlercrab_corneas.yaml` file. Alter the parameters to what you think are suitable

Change the `label_suffix` parameter in your config file to the name of the label that you want to detect. E.g. 'corneas' or 'rhabdoms'.

Choose a voxel spacing that you generated in step 3 to use in your config file by adding the path to these folders to the `train_images_dir`, `train_labels_dir`, `test_images_dir` and `test_labels_dir` in your config file.
E.g.
```
train_labels_dir: ./dataset/fiddlercrab_corneas/cropped/train_images_10
```
for a voxel spacing of 10.


5. Check whether the voxel spacing and heatmap parameters look suitable by inspecting plots generated by the following command (replacing `YOUR_CONFIG_FILE` with the name of your newly created config file):

```bash
python check_data.py ./configs/YOUR_CONFIG_FILE.yaml
```

If they are not suitable, try a different voxel spacing for `train_images_dir`, `train_labels_dir`, `test_images_dir` and `test_labels_dir` or different
`starting_sigma`, `peak_min_val` and `correct_prediction_distance` in your config file. Then, once modified, run the command
again.

You could also open these images with a 3d volume viewer (e.g. 3DSlicer or Dragonfly) and see what resampled resolution is suitable for detecting your features of interest.

One thing to consider in this decision is that a image/volume with too few voxels may not provide enough information for the model to detect the
feature, whereas a image with too many voxels may have so much information that it cannot be loaded into your computer's memory, or require an
unreasonably large training time or model size. If you face memory usage issues during training or inference, consider reducing the voxel spacing used.
You will also want your gaussian distributions in the heatmap to cover the object of interest (i.e. not be too narrow or broad). These can also
be parameters you optimise with hyperparameter tuning.

It's also good to ensure that all images and labels can be loaded without error using the `--check-loading` flag
of `check_data.py` like so (if you didn't plot every image and label above):

```bash
python check_data.py ./configs/YOUR_CONFIG_FILE.yaml --check-loading
```


### Train

6. Start training, specifying the path to your config file as an argument
```bash
python main.py train configs/fiddlercrab_corneas.yaml
```

(To view live training progress charts, open a new terminal in this directory and start up tensorboard)
```bash
source venv/bin/activate
tensorboard --logdir logs/fiddlercrab_corneas
```

Once you are happy with a model's performance, copy and paste its folder (in the logs/YOUR_CONFIG_FILENAME/lightning_logs directory
(e.g. version_5) to the the `zoo/` folder. This is so you can keep track of your models in a zoo, and easily
use them for running inference.

Its advisable that you change the `debug_plots` variable in your `config.yaml` file to `True` before running your full training length,
so that you can verify whether your parameters to locate peaks are suitable. Pay particular attention to the `peak_min_distance` and
`peak_min_val` variables. After you've verified everything looks ok, change `debug_plots` back to `False`.


### Optimise hyperparameters

7. (OPTIONAL) If you are unhappy with your models performance, it may be that the hyperparameters you are using, are not well suited
to your problem.

You can either:

- manually adjust these in the `config.yaml` file, and rerun training step 8 above.

- or, automatically optimise these parameters with a hyperparameter search:
```
python main.py tune configs/fiddlercrab_corneas.yaml
```
I have implemented what the search method in the `main.py` file and what hyperparameters to search for
in the `actions.py` file, under the `objective()` function. You can edit this function to change
what values you want to search for. See
https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize for more info.

If your computer can run multiple instances of the hyperparameter search/tuning process, open more terminals and type
```
source venv/bin/activate
python main.py tune configs/fiddlercrab_corneas.yaml -s sqlite:///fiddlercrab_corneas_tuning.db
```
to run the job in a parallised way and make the search process faster.

(To view live training progress charts, open a new terminal in this directory and start up tensorboard)
```bash
source venv/bin/activate
tensorboard --logdir lightning_logs
```

Once you have found suitable hyperparameters, and as you find better hyperparameters, they will be printed to the console
and also saved with their performance in a sqlite database. You should take these best values and use them to update your `config.yaml` file.


### Inference

8. Once you have a trained model, you can use it to make predictions, otherwise known as inference.

To do so, you will first have to resample the test volume to have approximately the same number of voxels between the labels
you are looking to detect as the images used for training. If you ran step 3, these will have already been generated for you
in a folder called something like: `data/fiddler/whole/test_images/`

Then run the following command, specifying the paths to your config file, the volume you want to run inference on and your trained model.

```
python main.py infer configs/fiddlercrab_corneas.yaml -v ./dataset/fiddlercrab_corneas/whole/test_images_10/ -m ./zoo/fiddlercrab_corneas/version_4/
```

Outputs from your inference will be found in the ./output directory.


## File overview
```
.
├── data_info.csv  - details where annotated MATLAB files and corresponding volumes are stored
├── dataset/  - the local copy of the dataset used for analysis. This should likely be symlinked to a data drive
├── generate_dataset.py  - a script to generate your dataset
├── landmarks.npy  - a file created for histogram standardisation by torchio_data_transform.ipynb
├── lightning_logs/  - where model trial files are stored
├── main.py  - the main script to run for training, testing and inference
├── deep_radiologist/  - helper classes and functions used specifically for this project
├── old_development_stuff/  - old files used during prototyping
├── output.png  - an example output of the model
├── README.md
├── remove_empty_training_data.py  - an experimental option to remove data with no labels
├── requirements.txt
├── scripts/  - helpful bash scripts for setup
└── torchio_data_transform.ipynb  - a file used to explore transformations of the data and generate the landmarks.npy file for histogram standardisation
```


## Paper methods

## Dataset
- num images, how they were sourced and how many subvolumes, batch size, number of patches used for training with augmentation (list some totals?).

### Preprocessing
- We resampled whole images to a resolution low enough to load into computer memory but high enough to clearly resolve features of interest when visually inspected in a 3d volume viewer. To do so, we scaled the resolution to alter the average number of voxels between features of interest. This allowed us to include volume data with variable spatial scales with or without spatial information. Final resolutions of datasets in average number of voxels between features were 10 for fiddlercrab corneas and fiddlercrab rhabdoms, 30? for paraphronima corneas and 20? for paraphronima rhabdoms. Labels were scaled to the same resolution as the images.

- Images were then cropped and cut into subvolumes to prepare for training.
- Because some images were partially labelled, we cropped images around labelled regions with a margin of 16 voxels.
- We then cut images and labels into smaller 256x256x256 voxel subvolumes to performantly load during training. Subvolumes with no labels were removed from the training data. These both additionally acted as measures to reduce false negative labels in the dataset. 

- Using deep learning models with 3d images requires additional memory considerations. The number of pixels in 2d image applications is rarely larger than X (references of popular models with their number of pixels). However, 3D images, due to their added dimension of information and common need to capture small detail, can contain hundreds of millions of voxels. In these cases, downsampling alone to fit memory requirements is often not acceptable when features of interest are identifiable with small details. However, patch-based sampling can be used.

- Patches were randomly sampled from the 256x256x256

### Data augmentation

##  


### Hyperparameter tuning

Each model was trained with a different set of hyperparameters. We employed a random search method with hyperband pruning to minimise the number of failures in trained models (see equation X). Each model underwent 50 search trials with each trial lasting 70 epochs. Hyperparameters used in final models are shown in Table X.

