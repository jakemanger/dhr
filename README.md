# DHR - Deep heatmap regression
Use a convolutional neural network (u-net) to do heatmap regression.

This project is set up to automatically localise one category of points in large 3D volumes,
but can be extended for other applications. Please [create an issue](https://github.com/jakemanger/dhr/issues/new)if you are facing issues and [submit a pull request](https://github.com/jakemanger/dhr/pulls) if you would like to contribute!


## Setup

1. Clone this repository from github

2. Install python3.9 (sometimes called `py` in the below command on windows) and make a python virtual environment in the root directory (if not already present).
```bash
python3.9 -m venv venv
```
See [here](https://towardsdatascience.com/getting-started-with-python-virtual-environments-252a6bd2240) 
for more information on virtual environments.

2. Activate the python virtual environment.
(On linux/macos)
```bash
source venv/bin/activate
```

(On Windows)
```
venv\Scripts\activate
```

3. Install dependencies
```bash
pip install -r requirements.txt
```

## Quick start (for linux)

1. If you haven't already, activate the python virtual environment.
```bash
source venv/bin/activate
```

### Prepare your dataset

2. Add the files you want to use for images and labels to a data source specifier csv file with 3 columns: 
- `image_file_path` (path to the dicom or nifti file used to annotate with mctv)
- `split` (containing 'train' or 'test' to say how the data should be split)
		This should be generated randomly, or rerun multiple times as part of a
		k-fold cross validation process.
- `labels_<YOUR_LABEL_NAME>` a path to a 3 column csv file (x, y and z axes) with the location of the labels you wish to detect.
    Add additional `labels_<YOUR_ADDITIONAL_LABEL_NAME>` columns if you wish to use these columns for defining
    a cropping area.
    
Example:
		data_source_specifiers/fiddlercrab_corneas.csv

Which should provide a csv file like the following:

| image_file_path | split | labels_corneas |
|-----------------|-------|----------------|
| /path/to/image1 | train | /path/to/cornea_labels1.csv |
| /path/to/image2 | test  | /path/to/cornea_labels2.csv |
| /path/to/image3 | train | /path/to/cornea_labels3.csv |


This file should be placed in the ./data_source_specifiers directory.

Note, if you have labelled your volumes in matlab using the mctv program, see [here](./docs/mctv_to_csv.md) to generate your label files and update your data source specifier csv file.


3. Generate the dataset (if not already found in the `./dataset/` directory) using the data source specifier csv file from step 2.
```bash
python generate_dataset.py ./data_source_specifiers/fiddlercrab_corneas.csv -l corneas -v 10 -cl corneas
```
*The `generate_dataset.py` command used above will have
generated patches and whole volumes for inference using a resolution that has on average 10 voxels between each feature of interest.
You can edit the previous command if you want to different resolutions by adding aguments after the `-v` flag).*


4. Create a new config file in the configs directory. You should create a new one of these for each of your tasks. You can base these on one of the examples: e.g. the `configs/fiddlercrab_corneas.yaml` file. Alter the parameters to what you think are suitable

Change the `label_suffix` parameter in your config file to the name of the label that you want to detect. E.g. 'corneas' or 'rhabdoms'.

Add the path to the dataset folders generated in step 3. You should update the `train_images_dir`, `train_labels_dir`, `test_images_dir` and `test_labels_dir` parameters.
E.g.
```
train_labels_dir: ./dataset/fiddlercrab_corneas/cropped/train_images_10
```


5. Check whether the voxel spacing, patch size (if you are using patches) and heatmap parameters look suitable by inspecting plots generated by the following command (replacing `YOUR_CONFIG_FILE` with the name of your newly created config file):

```bash
python check_data.py ./configs/YOUR_CONFIG_FILE.yaml
```
If you want to go though and check each image, this will plot each image and label in the dataset. This is useful for checking that the labels are oriented correctly and assigned to the right scan/image.

If they are not suitable, try a different voxel spacing for `train_images_dir`, `train_labels_dir`, `test_images_dir` and `test_labels_dir` or different
`starting_sigma`, `peak_min_val` and `correct_prediction_distance` in your config file. You should also make the decision as to whether you want to load your dataset
in smaller patches or just use the whole volumes (in the `./dataset/fiddlercrab_corneas/whole/train_images_10` directory in our example). A good reason to use whole 
volumes is if generate_dataset.py gave you a warning, loading time of your images is fast or if the cropped patches barely reduce the size of your scan.
Once modified, run the command again. By default, if a scan was larger than 256x256x256 voxels, it would have been cropped into smaller patches. Otherwise, it would have been loaded as a whole volume.
In this case, you can use the 'patches' directory for training and everything should be optimised for you. You can also use the 'whole' directory if you want to load the whole volumes (e.g. for evaluation).

You could also open these images with a 3d volume viewer (e.g. 3DSlicer or Dragonfly) and see what resampled resolution is suitable for detecting your features of interest.

One thing to consider in this decision is that a image/volume with too few voxels may not provide enough information for the model to detect the
feature, whereas a image with too many voxels may have so much information that it cannot be loaded into your computer's memory, or require an
unreasonably large training time or model size. If you face memory usage issues during training or inference, consider reducing the voxel spacing used.
You will also want your gaussian distributions in the heatmap to cover the object of interest (i.e. not be too narrow or broad). These can also
be parameters you optimise with hyperparameter tuning.

It's also good to ensure that all images and labels can be loaded without error using the `--check-loading` flag
of `check_data.py` like so (if you didn't plot every image and label above):

```bash
python check_data.py ./configs/YOUR_CONFIG_FILE.yaml --check-loading
```


### Train

*TIP run `python main.py -h` to view the help file for all available arguments and usage.*


6. Start training, specifying the path to your config file as an argument
```bash
python main.py train configs/fiddlercrab_corneas.yaml
```

(To view live training progress charts, open a new terminal in this directory and start up tensorboard)
```bash
source venv/bin/activate
tensorboard --logdir logs/fiddlercrab_corneas
```

Once you are happy with a model's performance, copy and paste its folder in the logs/YOUR_CONFIG_FILENAME/lightning_logs directory
(e.g. version_5) to the `zoo/` folder. This is so you can keep track of your models in a zoo, and easily
use them for running inference.

Its advisable that you change the `debug_plots` variable in your `config.yaml` file to `True` before running your full training length,
so that you can verify whether your parameters to locate peaks are suitable. Pay particular attention to the 
`peak_min_val` variable. After you've verified everything looks ok, change `debug_plots` back to `False` and train your model!


### Train with transfer learning or continue training

7. (OPTIONAL) If you would like to start with a pre-trained model and refine it on a new dataset, you can use transfer learning. You can also use this to continue training a model from a previous checkpoint.
The training process is the same, except you must specify the `starting_weights_path` argument with the `--starting_weights_path` or `-w`
flag.
For example:
```bash
python main.py train configs/fiddlercrab_corneas.yaml -w zoo/fiddlercrab_corneas/version_2/checkpoints/epoch=44-step=391680.ckpt
```
*Warning, the config used to train the starting weights (found in the same directory as the checkpoints folder) must have a
matching neural network architecture (e.g. number of layers and neurons) for this to work correctly. You should also start from a checkpoint
with a specific epoch number to ensure that the model loaded every volume in the dataset each epoch (for correct reporting of results).*


### Optimise hyperparameters

8. (OPTIONAL) If you are unhappy with your models performance, it may be that the hyperparameters you are using are not well suited
to your problem.

You can either:

- manually adjust these in the `config.yaml` file, and rerun training step 6 above.

- or, automatically optimise these parameters with a hyperparameter search:
```
python main.py tune configs/fiddlercrab_corneas.yaml
```
I have implemented what the search method in the `main.py` file and what hyperparameters to search for
in the `actions.py` file, under the `objective()` function. You can edit this function to change
what values you want to search for. See
https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize for more info.

If your computer can run multiple instances of the hyperparameter search/tuning process, open more terminals and type
```
source venv/bin/activate
python main.py tune configs/fiddlercrab_corneas.yaml -s sqlite:///fiddlercrab_corneas_tuning.db
```
to run the job in a parallised way and make the search process faster.

(To view live training progress charts, open a new terminal in this directory and start up tensorboard)
```bash
source venv/bin/activate
tensorboard --logdir lightning_logs
```

Once you have found suitable hyperparameters, and as you find better hyperparameters, they will be printed to the console
and also saved with their performance in a sqlite database. You should take these best values and use them to update your `config.yaml` file.


### Inference

9. Once you have a trained model, you can use it to make predictions, otherwise known as inference.

To do so, you will first have to resample the test volume to have approximately the same number of voxels between the labels
you are looking to detect as the images used for training. If you ran step 3, these will have already been generated for you
in a folder called something like: `data/fiddler/whole/test_images/`

Then run the following command, specifying the paths to your config file, the volume you want to run inference on and your trained model.

```bash
python main.py infer configs/fiddlercrab_corneas.yaml -v ./dataset/fiddlercrab_corneas/whole/test_images_10/ -m ./zoo/fiddlercrab_corneas/version_4/
```

Outputs from your inference will be found in the ./output directory.


## File overview
```
.
├── data_info.csv  - details where annotated MATLAB files and corresponding volumes are stored
├── dataset/  - the local copy of the dataset used for analysis. This should likely be symlinked to a data drive
├── generate_dataset.py  - a script to generate your dataset
├── landmarks.npy  - a file created for histogram standardisation by torchio_data_transform.ipynb
├── lightning_logs/  - where model trial files are stored
├── main.py  - the main script to run for training, testing and inference
├── deep_radiologist/  - helper classes and functions used specifically for this project
├── old_development_stuff/  - old files used during prototyping
├── output.png  - an example output of the model
├── README.md
├── remove_empty_training_data.py  - an experimental option to remove data with no labels
├── requirements.txt
├── scripts/  - helpful bash scripts for setup
└── torchio_data_transform.ipynb  - a file used to explore transformations of the data and generate the landmarks.npy file for histogram standardisation
```