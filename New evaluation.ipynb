{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In this document, we will analyse our models performance on the training data.\n",
    "\n",
    "We will test the effect of three different parameters:\n",
    "\n",
    "- Number of orientations (1, 2 or 3)\n",
    "- Prediction accumulation function (mean or mean > 0.5)\n",
    "- Peak detection function (center of mass or max filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Let's first look at the help text for how to use the python script for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py infer -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Now let's run all model combinations using a handy bash script at scripts/infer_with_all_models.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash scripts/infer_with_all_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Compare their performance against the labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_prefix = './'\n",
    "\n",
    "# config_path = './configs/fiddlercrab_corneas.yaml'\n",
    "# y_path = './dataset/fiddlercrab_corneas/whole/test_labels_10/dampieri_male_16-corneas.csv'\n",
    "# y_hat_path = 'output/dampieri_male_16-image.logs_fiddlercrab_corneas_lightning_logs_version_26_checkpoints_last_x_3_y_3_z_3_average_threshold_0.5_prediction_peak_min_val_0_5_method_center_of_mass.resampled_space_peaks.csv'\n",
    "# mct_path = './dataset/fiddlercrab_corneas/whole/test_images_10/dampieri_male_16-image.nii'\n",
    "\n",
    "config_path = './configs/paraphronima_corneas_without_random_rotation.yaml'\n",
    "y_path = './dataset/paraphronima_corneas/whole/test_labels_10//P_crassipes_FEG190213_003b_02_head-corneas.csv'\n",
    "y_hat_path = './output/P_crassipes_FEG190213_003b_02_head-image.logs_paraphronima_corneas_without_random_scale_lightning_logs_version_3_checkpoints_last_x_3_y_3_z_3_average_threshold_0.5_prediction_peak_min_val_0_25_method_center_of_mass.resampled_space_peaks.csv'\n",
    "mct_path = './dataset/paraphronima_corneas/whole/test_images_10//P_crassipes_FEG190213_003b_02_head-image.nii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_radiologist.lightning_modules import Model\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import numpy as np\n",
    "import torchio as tio\n",
    "from itables import init_notebook_mode\n",
    "\n",
    "init_notebook_mode(all_interactive=True)\n",
    "\n",
    "\n",
    "\n",
    "# load the coordinates to analyse\n",
    "def load_coordinates(path, flip_axes=False, mct_path=None):\n",
    "    locations = np.loadtxt(\n",
    "        path,\n",
    "        delimiter=',',\n",
    "        ndmin=2,\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    if not flip_axes: \n",
    "        return locations.tolist()\n",
    "\n",
    "    if not mct_path:\n",
    "        raise Exception('You must specify `mct_path` if you need to flip_axes')\n",
    "\n",
    "    mct = tio.ScalarImage(mct_path)\n",
    "    locations[:,0] = mct.shape[1] - locations[:,0]\n",
    "    locations[:,1] = mct.shape[2] - locations[:,1]\n",
    "\n",
    "    return locations.tolist()\n",
    "\n",
    "\n",
    "y_hat = load_coordinates(y_hat_path, flip_axes=True, mct_path=mct_path)\n",
    "y = load_coordinates(y_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Let's plot the y vs y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot(*datasets, colors, labels=None, backend='matplotlib'):\n",
    "    if backend == 'matplotlib':\n",
    "        # Create a 3D plot using Matplotlib\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        # Iterate over datasets and corresponding colors\n",
    "        for i, (data, color) in enumerate(zip(datasets, colors)):\n",
    "            # Unpack the data into x, y, z coordinates\n",
    "            x = [point[0] for point in data]\n",
    "            y = [point[1] for point in data]\n",
    "            z = [point[2] for point in data]\n",
    "            \n",
    "            # Plot each dataset\n",
    "            ax.scatter(x, y, z, color=color, label=f'dataset_{i+1}' if labels is None else labels[i])\n",
    "        \n",
    "        # Label the axes\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "        \n",
    "        # Add a legend\n",
    "        ax.legend()\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        return fig\n",
    "    elif backend == 'plotly':\n",
    "        # Create a 3D plot using Plotly\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Iterate over datasets and corresponding colors\n",
    "        for i, (data, color) in enumerate(zip(datasets, colors)):\n",
    "            # Unpack the data into x, y, z coordinates\n",
    "            x = [point[0] for point in data]\n",
    "            y = [point[1] for point in data]\n",
    "            z = [point[2] for point in data]\n",
    "            \n",
    "            # Plot each dataset\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=x, y=y, z=z,\n",
    "                mode='markers',\n",
    "                marker=dict(color=color, size=2),\n",
    "                name=f'dataset_{i+1}' if labels is None else labels[i]\n",
    "            ))\n",
    "\n",
    "        # Label the axes\n",
    "        fig.update_layout(\n",
    "            scene=dict(\n",
    "                xaxis_title='X',\n",
    "                yaxis_title='Y',\n",
    "                zaxis_title='Z'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config_path, y, y_hat):\n",
    "    # get the accuracy metrics for this image\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "    # override correct_prediction_distance to something more appropriate\n",
    "    config[\"correct_prediction_distance\"] = 15\n",
    "    \n",
    "    model = Model(config)\n",
    "    \n",
    "    tp, fp, fn, loc_err = model._get_acc_metrics(y_hat, y)\n",
    "    tps, fps, fns = model._get_acc_metrics(y_hat, y, return_coords=True)\n",
    "    \n",
    "    return tp, fp, fn, loc_err, tps, fps, fns\n",
    "\n",
    "tp, fp, fn, loc_err, tps, fps, fns = evaluate(config_path, y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True positives:', tp)\n",
    "print('False positives:', fp)\n",
    "print('False negatives:', fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    tps,\n",
    "    fps,\n",
    "    fns,\n",
    "    colors=['green', 'blue', 'red'],\n",
    "    labels=['True positive', 'False positives', 'False negatives'],\n",
    "    backend='plotly'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "That looks like a great result. Now let's run it for the rest of the scans/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Evaluate all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "\n",
    "# folder_path = './output/'\n",
    "folder_path = './output/'\n",
    "\n",
    "\n",
    "substrings = [\n",
    "  \"fiddlercrab_corneas_lightning_logs_version_26\",\n",
    "  \"fiddlercrab_rhabdoms_lightning_logs_version_10\",\n",
    "  \"paraphronima_corneas_lightning_logs_version_11\",\n",
    "  \"paraphronima_rhabdoms_lightning_logs_version_23\",\n",
    "  \"paraphronima_rhabdoms_without_target_heatmap_masking_lightning_logs_version_1\",\n",
    "  \"fiddlercrab_rhabdoms_with_target_heatmap_masking_lightning_logs_version_2\",\n",
    "  \"paraphronima_rhabdoms_without_elastic_deformation_lightning_logs_version_2\",\n",
    "  \"paraphronima_corneas_without_elastic_deformation_lightning_logs_version_2\",\n",
    "  \"fiddlercrab_corneas_without_elastic_deformation_lightning_logs_version_2\",\n",
    "  \"fiddlercrab_rhabdoms_without_elastic_deformation_lightning_logs_version_3\",\n",
    "  \"paraphronima_rhabdoms_without_random_scale_lightning_logs_version_3\",\n",
    "  \"paraphronima_corneas_without_random_scale_lightning_logs_version_3\",\n",
    "  \"fiddlercrab_rhabdoms_without_random_scale_lightning_logs_version_3\",\n",
    "  \"paraphronima_rhabdoms_without_random_rotation_lightning_logs_version_2\",\n",
    "  \"paraphronima_corneas_without_random_rotation_lightning_logs_version_1\",\n",
    "  \"fiddlercrab_corneas_without_random_rotation_lightning_logs_version_2\",\n",
    "  \"fiddlercrab_rhabdoms_without_random_rotation_lightning_logs_version_0\",\n",
    "  \"fiddlercrab_corneas_hist_std_lightning_logs_version_0\",\n",
    "  \"fiddlercrab_rhabdoms_hist_std_lightning_logs_version_0\",\n",
    "  \"fiddlercrab_corneas_random_affine_prop_1_lightning_logs_version_0\",\n",
    "  \"fiddlercrab_corneas_hist_std_z_norm_lightning_logs_version_0\",\n",
    "  \"fiddlercrab_rhabdoms_hist_std_z_norm_lightning_logs_version_0\"\n",
    "]\n",
    "\n",
    "for substring in substrings:\n",
    "\n",
    "    def find_files_with_substring_and_suffix(folder_path, substring, suffix, other_words):\n",
    "        matching_files = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if substring in filename and filename.endswith(suffix) and other_words in filename:\n",
    "                matching_files.append(filename)\n",
    "        return matching_files\n",
    "\n",
    "    def split_filenames_by_hyphen(filenames):\n",
    "        split_filenames = [filename.split('-')[0] for filename in filenames]\n",
    "        return split_filenames\n",
    "\n",
    "    suffix = 'resampled_space_peaks.csv'\n",
    "    files = find_files_with_substring_and_suffix(folder_path, substring, suffix, 'x_3_y_3_z_3')\n",
    "    split_files = split_filenames_by_hyphen(files)\n",
    "\n",
    "    print(files[:3])\n",
    "    print(split_files[:3])\n",
    "    print(len(files))\n",
    "\n",
    "    prefix = substring.split('_lightning')[0]\n",
    "    feature_to_find = 'rhabdoms' if 'rhabdoms' in prefix else 'corneas'\n",
    "\n",
    "    config_file = f'./configs/{prefix}.yaml'\n",
    "\n",
    "    # read the config\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    config_paths = [config_file for file in files]\n",
    "    y_paths = [f'{config[\"test_labels_dir\"]}/{name}-{config[\"label_suffix\"]}.csv'.replace(\"patches\", \"whole\") for name in split_files]\n",
    "    y_hat_paths = [f'./output/{file}' for file in files]\n",
    "    mct_paths = [f'{config[\"test_images_dir\"]}/{name}-image.nii'.replace(\"patches\", \"whole\") for name in split_files]\n",
    "\n",
    "    print(config_paths[:5])\n",
    "    print(y_paths[:5])\n",
    "    print(y_hat_paths[:5])\n",
    "    print(mct_paths[:5])\n",
    "    len(mct_paths)\n",
    "\n",
    "    plot_results = False\n",
    "\n",
    "    for config_path, y_path, y_hat_path, mct_path in zip(config_paths, y_paths, y_hat_paths, mct_paths):\n",
    "        results = []\n",
    "\n",
    "        y_hat = load_coordinates(y_hat_path, flip_axes=True, mct_path=mct_path)\n",
    "        y = load_coordinates(y_path)\n",
    "\n",
    "        tp, fp, fn, loc_err, tps, fps, fns = evaluate(config_path, y, y_hat)\n",
    "\n",
    "        print('True positives:', tp)\n",
    "        print('False positives:', fp)\n",
    "        print('False negatives:', fn)\n",
    "        \n",
    "        if plot_results:\n",
    "            plot(\n",
    "                tps,\n",
    "                fps,\n",
    "                fns,\n",
    "                colors=['green', 'blue', 'red'],\n",
    "                labels=['True positive', 'False positives', 'False negatives'],\n",
    "                backend='plotly'\n",
    "            )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                'mct_path': mct_path,\n",
    "                'y_path': y_path,\n",
    "                'y_hat_path': y_hat_path,\n",
    "                'config_path': config_path,\n",
    "                'num_tps': tp,\n",
    "                'num_fps': fp,\n",
    "                'num_fns': fn,\n",
    "                'tps': tps,\n",
    "                'fps': fps,\n",
    "                'fns': fns\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        file_name_without_extension = os.path.splitext(os.path.basename(y_hat_path))[0]\n",
    "    \n",
    "        print(f'saving pickle to ./analysis_output/{file_name_without_extension}_results.pickle')\n",
    "        with open(f'./analysis_output/{file_name_without_extension}_results.pickle', 'wb') as handle:\n",
    "            pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#     if 'num_tps' not in results:\n",
    "#         print('couldnt find results')\n",
    "#         continue\n",
    "        # specify the keys you want to keep\n",
    "        keys_to_keep = ['mct_path', 'config_path', 'y_path', 'y_hat_path', 'num_tps', 'num_fps', 'num_fns']\n",
    "\n",
    "        # create a new list of dictionaries with only the specified keys\n",
    "        filtered_data = [{k: v for k, v in d.items() if k in keys_to_keep} for d in results]\n",
    "\n",
    "        df = pd.DataFrame(filtered_data)\n",
    "\n",
    "\n",
    "        # Calculate precision\n",
    "        precision = df['num_tps'] / (df['num_tps'] + df['num_fps'])\n",
    "\n",
    "        # Calculate recall\n",
    "        recall = df['num_tps'] / (df['num_tps'] + df['num_fns'])\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        f1_score\n",
    "\n",
    "        # add f1 score\n",
    "        df['precision'] = precision\n",
    "        df['recall'] = recall\n",
    "        df['f1'] = f1_score\n",
    "\n",
    "        df.to_csv(f'./analysis_output/{file_name_without_extension}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Let's make a table to compare the inference approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "import os\n",
    "import glob\n",
    "\n",
    "all_files = glob.glob(\"./analysis_output/*.csv\")\n",
    "\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
    "pickles = glob.glob(\"./analysis_output/*.pickle\")\n",
    "\n",
    "# create a dictionary mapping basenames of pickle files to their full path\n",
    "pickle_map = {'.'.join(os.path.basename(pickle).rsplit('.', 2)[:-2]): pickle for pickle in pickles}\n",
    "\n",
    "def link_pickle_to_y_hat(y_hat_path):\n",
    "    ''' Extract the basename from the 'y_hat_path' and use it to find the corresponding pickle file '''\n",
    "    y_hat_basename = os.path.basename(y_hat_path)\n",
    "    y_hat_basename = '.'.join(y_hat_basename.rsplit('.', 2)[:-2])\n",
    "    return pickle_map.get(y_hat_basename, None)  # Returns None if no match is found\n",
    "\n",
    "# Apply the function to the 'y_hat_path' column to populate the 'pickle' column\n",
    "df['pickle'] = df['y_hat_path'].apply(link_pickle_to_y_hat)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude groups where all 'f1' values are NaN\n",
    "valid_groups = df.groupby('config_path')['f1'].transform('max').notna()\n",
    "df = df[valid_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'config_path' and get the index of the row with the max 'f1' score for each group\n",
    "idx = df.groupby(['config_path', 'mct_path'])['f1'].idxmax()\n",
    "\n",
    "# Filter the aDataFrame to keep only those rows\n",
    "# best_df = df.loc[idx].reset_index(drop=True)\n",
    "\n",
    "# Sort the filtered DataFrame by 'recall' in descending order (highest first)\n",
    "best_df = df.sort_values(by='f1', ascending=False)\n",
    "\n",
    "best_df.to_csv('best_models.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the filtered DataFrame\n",
    "print(best_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from plotly import io as pio\n",
    "\n",
    "# List to hold the table rows\n",
    "table_data = []\n",
    "\n",
    "for index, row in best_df.iterrows():\n",
    "    with open(row['pickle'], 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    # Ensure the data has only one dictionary\n",
    "    assert len(data) == 1, 'there should only be one dictionary in each pickle'\n",
    "    data = data[0]\n",
    "\n",
    "    # Create the plot using Plotly\n",
    "    fig = plot(\n",
    "        data['tps'],\n",
    "        data['fps'],\n",
    "        data['fns'],\n",
    "        colors=['green', 'blue', 'red'],\n",
    "        labels=['True positive', 'False positives', 'False negatives'],\n",
    "        backend='plotly'\n",
    "    )\n",
    "\n",
    "    # Save the plot as an image\n",
    "    image_filename = os.path.abspath(f\"./analysis_output/plot_{index}.png\")\n",
    "    pio.write_image(fig, image_filename, format='png')\n",
    "\n",
    "    # Store plot and data\n",
    "    table_data.append({\n",
    "        'Config Path': row['config_path'],\n",
    "        'Scan Path': row['mct_path'],\n",
    "        'Num TPS': data['num_tps'],\n",
    "        'Num FPS': data['num_fps'],\n",
    "        'Num FNS': data['num_fns'],\n",
    "        #'Precision': data['precision'],\n",
    "        'Recall': row['recall'],\n",
    "        'F1': row['f1'],\n",
    "        'Plot': image_filename  # Save the path to the image\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "table_df = pd.DataFrame(table_data)\n",
    "\n",
    "# Save to an Excel file and embed images\n",
    "excel_filename = \"best_models_table.xlsx\"\n",
    "table_df.drop(columns=[\"Plot\"], inplace=False).to_excel(excel_filename, sheet_name='Best Models', index=False)\n",
    "\n",
    "# Load workbook to insert images\n",
    "workbook = load_workbook(excel_filename)\n",
    "worksheet = workbook['Best Models']\n",
    "\n",
    "for i, row in enumerate(table_df.itertuples(index=False), start=2):  # Start from row 2 in Excel\n",
    "    image_filename = row.Plot\n",
    "    if os.path.exists(image_filename):\n",
    "        img = Image(image_filename)\n",
    "        img.height = 150  # Resize image if necessary\n",
    "        img.width = 150\n",
    "        cell = f'H{i}'  # Column H corresponds to \"Plot\"\n",
    "        worksheet.add_image(img, cell)\n",
    "\n",
    "# Save the workbook\n",
    "workbook.save(excel_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Now let's plot these results to see what is the best method of inference.\n",
    "We will look at the number of orientations, the peak_min_val and the min val threshold when combining multiple orientations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def extract_info(path):\n",
    "    version = re.search(r'version_(\\d+)', path)\n",
    "    x_num = re.search(r'_x_(\\d+)', path)\n",
    "    y_num = re.search(r'_y_(\\d+)', path)\n",
    "    z_num = re.search(r'_z_(\\d+)', path)\n",
    "    avg_threshold = re.search(r'average_threshold_([\\d.]+)', path)\n",
    "    peak_min_val = re.search(r'peak_min_val_(\\d+_\\d+)', path)\n",
    "    method = re.search(r'method_([a-zA-Z_]+)\\.', path)\n",
    "    \n",
    "    # Extract the groups if found, otherwise return NaN\n",
    "    version = version.group(1) if version else np.nan\n",
    "    x_num = int(x_num.group(1)) if x_num else np.nan\n",
    "    y_num = int(y_num.group(1)) if y_num else np.nan\n",
    "    z_num = int(z_num.group(1)) if z_num else np.nan\n",
    "    avg_threshold = float(avg_threshold.group(1)) if avg_threshold else np.nan\n",
    "    peak_min_val = float(peak_min_val.group(1).replace('_', '.')) if peak_min_val else np.nan\n",
    "    method = method.group(1) if method else np.nan\n",
    "    \n",
    "    return pd.Series([version, x_num, y_num, z_num, avg_threshold, peak_min_val, method])\n",
    "\n",
    "\n",
    "df[['model_version', 'x_num', 'y_num', 'z_num', 'avg_threshold', 'peak_min_val', 'method']] = df['y_hat_path'].apply(extract_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df['num_orientations'] = df.z_num + df.y_num + df.z_num\n",
    "\n",
    "\n",
    "# Filter rows where 'y_hat' contains a specific substring, e.g., 'flammula'\n",
    "#filtered_df = df[df['y_path'].str.contains('dampieri', na=False)]\n",
    "#filtered_df = df[df['y_path'].str.contains('flammula', na=False)]\n",
    "filtered_df = df\n",
    "\n",
    "\n",
    "fig = px.scatter(filtered_df, x='avg_threshold', y='recall',\n",
    "              color='num_orientations')\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(filtered_df, x='peak_min_val', y='recall',\n",
    "              color='num_orientations')\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(filtered_df, x='peak_min_val', y='recall',\n",
    "              color='avg_threshold')\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter_3d(filtered_df, x='peak_min_val', y='num_orientations', z='recall',\n",
    "              color='method')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter_3d(filtered_df, x='avg_threshold', y='num_orientations', z='recall',\n",
    "              color='method')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter_3d(df, x='avg_threshold', y='num_orientations', z='recall',\n",
    "              color='y_path')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter_3d(df, x='peak_min_val', y='num_orientations', z='recall',\n",
    "              color='y_path')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "this shows:\n",
    "- max_filter and center_of_mass are good in different circumstances\n",
    "- more orientations is better (>5) but let's do 3, 3, 3\n",
    "- avg_threshold is better smaller (0.2) for flammula but greater (0.5) for dampieri\n",
    "- peak_min_val is better smaller (0.2) for flammula by greater (0.5) for dampieri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Let's find the best parameters for inference for both dampieri and flammula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_paths = pd.unique(df.mct_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results = True\n",
    "score_measures = ['f1', 'recall']\n",
    "\n",
    "for score_measure in score_measures:\n",
    "    for scan in unique_paths:\n",
    "        filtered_df = df[df.mct_path == scan]\n",
    "        best_i = filtered_df['f1'].idxmax()\n",
    "\n",
    "        print(f'The best model for {scan} according to {score_measure} is:\\n{filtered_df.y_hat_path[best_i]}\\n')\n",
    "\n",
    "\n",
    "        y_hat = load_coordinates(filtered_df.y_hat_path[best_i], flip_axes=True, mct_path=scan)\n",
    "        y = load_coordinates(filtered_df.y_path[best_i])\n",
    "\n",
    "        tp, fp, fn, loc_err, tps, fps, fns = evaluate(config_path, y, y_hat)\n",
    "\n",
    "        print('True positives:', tp)\n",
    "        print('False positives:', fp)\n",
    "        print('False negatives:', fn)\n",
    "\n",
    "        if plot_results:\n",
    "            plot(\n",
    "                tps,\n",
    "                fps,\n",
    "                fns,\n",
    "                colors=['green', 'blue', 'red'],\n",
    "                labels=['True positive', 'False positives', 'False negatives'],\n",
    "                backend='plotly'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "obj = pd.read_pickle(r'/home/jake/projects/dhr/fiddlercrab_corneas_lightning_logs_version_26_results.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Replace these paths with appropriate ones for your project\n",
    "data_list = obj\n",
    "\n",
    "def load_nifti_image(path):\n",
    "    \"\"\"Loads a NIfTI (.nii) image and returns it as a NumPy array.\"\"\"\n",
    "    img = nib.load(path)\n",
    "    return img.get_fdata()\n",
    "\n",
    "def plot_and_edit_points(data):\n",
    "    \"\"\"Loads the mct (multi-channel test image) and allows interactive point editing in Napari.\"\"\"\n",
    "    mct = load_nifti_image(data['mct_path'])\n",
    "\n",
    "    file_name_without_extension = os.path.splitext(os.path.basename(data['y_hat_path']))[0]\n",
    "    with napari.gui_qt():\n",
    "        viewer = napari.Viewer()\n",
    "        # Add the 3D image to the viewer\n",
    "        viewer.add_image(mct, name='MCT Image')\n",
    "\n",
    "        # Add true positives as points in green\n",
    "        tps_layer = viewer.add_points(data['tps'], size=5, edge_color='green', face_color='green', name='True Positives')\n",
    "\n",
    "        # Add false positives as points in red\n",
    "        fps_layer = viewer.add_points(data['fps'], size=5, edge_color='red', face_color='red', name='False Positives')\n",
    "\n",
    "        # Add false negatives as points in blue\n",
    "        fns_layer = viewer.add_points(data['fns'], size=5, edge_color='blue', face_color='blue', name='False Negatives')\n",
    "\n",
    "        # Enable editing for the points (tps, fps, fns)\n",
    "        tps_layer.editable = True\n",
    "        fps_layer.editable = True\n",
    "        fns_layer.editable = True\n",
    "\n",
    "        @viewer.bind_key('s')\n",
    "        def save_points(viewer):\n",
    "            \"\"\"Save edited points when 's' is pressed.\"\"\"\n",
    "            np.savetxt(f'{file_name_without_extension}_cleaned_tps.csv', tps_layer.data, delimiter=',', header='x,y,z', comments='')\n",
    "            np.savetxt(f'{file_name_without_extension}_cleaned_fps.csv', fps_layer.data, delimiter=',', header='x,y,z', comments='')\n",
    "            np.savetxt(f'{file_name_without_extension}_cleaned_fns.csv', fns_layer.data, delimiter=',', header='x,y,z', comments='')\n",
    "            print(\"Edited points saved.\")\n",
    "\n",
    "for item in data_list:\n",
    "    plot_and_edit_points(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
