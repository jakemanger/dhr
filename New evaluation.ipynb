{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In this document, we will analyse our models performance on the training data.\n",
    "\n",
    "We will test the effect of three different parameters:\n",
    "\n",
    "- Number of orientations (1, 2 or 3)\n",
    "- Prediction accumulation function (mean or mean > 0.5)\n",
    "- Peak detection function (center of mass or max filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Let's first look at the help text for how to use the python script for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python main.py infer -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Now let's run all model combinations using a handy bash script at scripts/infer_with_all_models.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash scripts/infer_with_all_models.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash scripts/infer_with_all_models.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash scripts/infer_with_all_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Compare their performance against the labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_prefix = './'\n",
    "\n",
    "# config_path = './configs/fiddlercrab_corneas.yaml'\n",
    "# y_path = './dataset/fiddlercrab_corneas/whole/test_labels_10/dampieri_male_16-corneas.csv'\n",
    "# y_hat_path = 'output/dampieri_male_16-image.logs_fiddlercrab_corneas_lightning_logs_version_26_checkpoints_last_x_3_y_3_z_3_average_threshold_0.5_prediction_peak_min_val_0_5_method_center_of_mass.resampled_space_peaks.csv'\n",
    "# mct_path = './dataset/fiddlercrab_corneas/whole/test_images_10/dampieri_male_16-image.nii'\n",
    "\n",
    "config_path = './configs/paraphronima_corneas_without_random_rotation.yaml'\n",
    "y_path = './dataset/paraphronima_corneas/whole/test_labels_10//P_crassipes_FEG190213_003b_02_head-corneas.csv'\n",
    "y_hat_path = './output/P_crassipes_FEG190213_003b_02_head-image.logs_paraphronima_corneas_without_random_scale_lightning_logs_version_3_checkpoints_last_x_3_y_3_z_3_average_threshold_0.5_prediction_peak_min_val_0_25_method_center_of_mass.resampled_space_peaks.csv'\n",
    "mct_path = './dataset/paraphronima_corneas/whole/test_images_10//P_crassipes_FEG190213_003b_02_head-image.nii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_radiologist.lightning_modules import Model\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import numpy as np\n",
    "import torchio as tio\n",
    "from itables import init_notebook_mode\n",
    "\n",
    "init_notebook_mode(all_interactive=True)\n",
    "\n",
    "\n",
    "\n",
    "# load the coordinates to analyse\n",
    "def load_coordinates(path, flip_axes=False, mct_path=None):\n",
    "    locations = np.loadtxt(\n",
    "        path,\n",
    "        delimiter=',',\n",
    "        ndmin=2,\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    if not flip_axes: \n",
    "        return locations.tolist()\n",
    "\n",
    "    if not mct_path:\n",
    "        raise Exception('You must specify `mct_path` if you need to flip_axes')\n",
    "\n",
    "    mct = tio.ScalarImage(mct_path)\n",
    "    locations[:,0] = mct.shape[1] - locations[:,0]\n",
    "    locations[:,1] = mct.shape[2] - locations[:,1]\n",
    "\n",
    "    return locations.tolist()\n",
    "\n",
    "\n",
    "y_hat = load_coordinates(y_hat_path, flip_axes=True, mct_path=mct_path)\n",
    "y = load_coordinates(y_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Let's plot the y vs y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot(*datasets, colors, labels=None, backend='matplotlib'):\n",
    "    if backend == 'matplotlib':\n",
    "        # Create a 3D plot using Matplotlib\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        # Iterate over datasets and corresponding colors\n",
    "        for i, (data, color) in enumerate(zip(datasets, colors)):\n",
    "            # Unpack the data into x, y, z coordinates\n",
    "            x = [point[0] for point in data]\n",
    "            y = [point[1] for point in data]\n",
    "            z = [point[2] for point in data]\n",
    "            \n",
    "            # Plot each dataset\n",
    "            ax.scatter(x, y, z, color=color, label=f'dataset_{i+1}' if labels is None else labels[i])\n",
    "        \n",
    "        # Label the axes\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "        \n",
    "        # Add a legend\n",
    "        ax.legend()\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        return fig\n",
    "    elif backend == 'plotly':\n",
    "        # Create a 3D plot using Plotly\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Iterate over datasets and corresponding colors\n",
    "        for i, (data, color) in enumerate(zip(datasets, colors)):\n",
    "            # Unpack the data into x, y, z coordinates\n",
    "            x = [point[0] for point in data]\n",
    "            y = [point[1] for point in data]\n",
    "            z = [point[2] for point in data]\n",
    "            \n",
    "            # Plot each dataset\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=x, y=y, z=z,\n",
    "                mode='markers',\n",
    "                marker=dict(color=color, size=2),\n",
    "                name=f'dataset_{i+1}' if labels is None else labels[i]\n",
    "            ))\n",
    "\n",
    "        # Label the axes\n",
    "        fig.update_layout(\n",
    "            scene=dict(\n",
    "                xaxis_title='X',\n",
    "                yaxis_title='Y',\n",
    "                zaxis_title='Z'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config_path, y, y_hat):\n",
    "    # get the accuracy metrics for this image\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "    # override correct_prediction_distance to something more appropriate\n",
    "    config[\"correct_prediction_distance\"] = 20\n",
    "    \n",
    "    model = Model(config)\n",
    "    \n",
    "    tp, fp, fn, loc_err = model._get_acc_metrics(y_hat, y)\n",
    "    tps, fps, fns = model._get_acc_metrics(y_hat, y, return_coords=True)\n",
    "    \n",
    "    return tp, fp, fn, loc_err, tps, fps, fns\n",
    "\n",
    "tp, fp, fn, loc_err, tps, fps, fns = evaluate(config_path, y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True positives:', tp)\n",
    "print('False positives:', fp)\n",
    "print('False negatives:', fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    tps,\n",
    "    fps,\n",
    "    fns,\n",
    "    colors=['green', 'blue', 'red'],\n",
    "    labels=['True positive', 'False positives', 'False negatives'],\n",
    "    backend='plotly'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "That looks like a great result. Now let's run it for the rest of the scans/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Evaluate all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "\n",
    "# folder_path = './output/'\n",
    "folder_path = './output/'\n",
    "\n",
    "\n",
    "substrings = [\n",
    "  \"fiddlercrab_corneas_lightning_logs_version_26\",\n",
    "  \"fiddlercrab_rhabdoms_lightning_logs_version_10\",\n",
    "  \"paraphronima_corneas_lightning_logs_version_11\",\n",
    "  \"paraphronima_rhabdoms_lightning_logs_version_23\",\n",
    "  \"paraphronima_rhabdoms_without_target_heatmap_masking_lightning_logs_version_1\",\n",
    "  \"fiddlercrab_rhabdoms_with_target_heatmap_masking_lightning_logs_version_2\",\n",
    "  \"paraphronima_rhabdoms_without_elastic_deformation_lightning_logs_version_2\",\n",
    "  \"paraphronima_corneas_without_elastic_deformation_lightning_logs_version_2\",\n",
    "  \"fiddlercrab_corneas_without_elastic_deformation_lightning_logs_version_2\",\n",
    "  \"fiddlercrab_rhabdoms_without_elastic_deformation_lightning_logs_version_3\",\n",
    "  \"paraphronima_rhabdoms_without_random_scale_lightning_logs_version_3\",\n",
    "  \"paraphronima_corneas_without_random_scale_lightning_logs_version_3\",\n",
    "  \"fiddlercrab_rhabdoms_without_random_scale_lightning_logs_version_3\",\n",
    "  \"paraphronima_rhabdoms_without_random_rotation_lightning_logs_version_2\",\n",
    "  \"paraphronima_corneas_without_random_rotation_lightning_logs_version_1\",\n",
    "  \"fiddlercrab_corneas_without_random_rotation_lightning_logs_version_2\",\n",
    "  \"fiddlercrab_rhabdoms_without_random_rotation_lightning_logs_version_0\",\n",
    "  \"fiddlercrab_corneas_hist_std_lightning_logs_version_0\",\n",
    "  \"fiddlercrab_rhabdoms_hist_std_lightning_logs_version_0\",\n",
    "  \"fiddlercrab_corneas_random_affine_prop_1_lightning_logs_version_0\",\n",
    "  \"fiddlercrab_corneas_hist_std_z_norm_lightning_logs_version_0\",\n",
    "  \"fiddlercrab_rhabdoms_hist_std_z_norm_lightning_logs_version_0\"\n",
    "]\n",
    "\n",
    "for substring in substrings:\n",
    "\n",
    "    def find_files_with_substring_and_suffix(folder_path, substring, suffix, other_words):\n",
    "        matching_files = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if substring in filename and filename.endswith(suffix) and other_words in filename:\n",
    "                matching_files.append(filename)\n",
    "        return matching_files\n",
    "\n",
    "    def split_filenames_by_hyphen(filenames):\n",
    "        split_filenames = [filename.split('-')[0] for filename in filenames]\n",
    "        return split_filenames\n",
    "\n",
    "    suffix = 'resampled_space_peaks.csv'\n",
    "    files = find_files_with_substring_and_suffix(folder_path, substring, suffix, 'x_3_y_3_z_3')\n",
    "    split_files = split_filenames_by_hyphen(files)\n",
    "\n",
    "    print(files[:3])\n",
    "    print(split_files[:3])\n",
    "    print(len(files))\n",
    "\n",
    "    prefix = substring.split('_lightning')[0]\n",
    "    feature_to_find = 'rhabdoms' if 'rhabdoms' in prefix else 'corneas'\n",
    "\n",
    "    config_file = f'./configs/{prefix}.yaml'\n",
    "\n",
    "    # read the config\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    config_paths = [config_file for file in files]\n",
    "    y_paths = [f'{config[\"test_labels_dir\"]}/{name}-{config[\"label_suffix\"]}.csv'.replace(\"patches\", \"whole\") for name in split_files]\n",
    "    y_hat_paths = [f'./output/{file}' for file in files]\n",
    "    mct_paths = [f'{config[\"test_images_dir\"]}/{name}-image.nii'.replace(\"patches\", \"whole\") for name in split_files]\n",
    "\n",
    "    print(config_paths[:5])\n",
    "    print(y_paths[:5])\n",
    "    print(y_hat_paths[:5])\n",
    "    print(mct_paths[:5])\n",
    "    len(mct_paths)\n",
    "\n",
    "    plot_results = False\n",
    "\n",
    "    for config_path, y_path, y_hat_path, mct_path in zip(config_paths, y_paths, y_hat_paths, mct_paths):\n",
    "        results = []\n",
    "\n",
    "        y_hat = load_coordinates(y_hat_path, flip_axes=True, mct_path=mct_path)\n",
    "        y = load_coordinates(y_path)\n",
    "\n",
    "        tp, fp, fn, loc_err, tps, fps, fns = evaluate(config_path, y, y_hat)\n",
    "\n",
    "        print('True positives:', tp)\n",
    "        print('False positives:', fp)\n",
    "        print('False negatives:', fn)\n",
    "        \n",
    "        if plot_results:\n",
    "            plot(\n",
    "                tps,\n",
    "                fps,\n",
    "                fns,\n",
    "                colors=['green', 'blue', 'red'],\n",
    "                labels=['True positive', 'False positives', 'False negatives'],\n",
    "                backend='plotly'\n",
    "            )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                'mct_path': mct_path,\n",
    "                'y_path': y_path,\n",
    "                'y_hat_path': y_hat_path,\n",
    "                'config_path': config_path,\n",
    "                'num_tps': tp,\n",
    "                'num_fps': fp,\n",
    "                'num_fns': fn,\n",
    "                'tps': tps,\n",
    "                'fps': fps,\n",
    "                'fns': fns\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        file_name_without_extension = os.path.splitext(os.path.basename(y_hat_path))[0]\n",
    "        # shorten if larger than 30 characters\n",
    "        if len(file_name_without_extension) > 30:\n",
    "            file_name_without_extension = file_name_without_extension[:30]\n",
    "    \n",
    "        print(f'saving pickle to ./analysis_output/{file_name_without_extension}_results.pickle')\n",
    "        with open(f'./analysis_output/{file_name_without_extension}_results.pickle', 'wb') as handle:\n",
    "            pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#     if 'num_tps' not in results:\n",
    "#         print('couldnt find results')\n",
    "#         continue\n",
    "        # specify the keys you want to keep\n",
    "        keys_to_keep = ['mct_path', 'config_path', 'y_path', 'y_hat_path', 'num_tps', 'num_fps', 'num_fns']\n",
    "\n",
    "        # create a new list of dictionaries with only the specified keys\n",
    "        filtered_data = [{k: v for k, v in d.items() if k in keys_to_keep} for d in results]\n",
    "\n",
    "        df = pd.DataFrame(filtered_data)\n",
    "\n",
    "\n",
    "        # Calculate precision\n",
    "        precision = df['num_tps'] / (df['num_tps'] + df['num_fps'])\n",
    "\n",
    "        # Calculate recall\n",
    "        recall = df['num_tps'] / (df['num_tps'] + df['num_fns'])\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        f1_score\n",
    "\n",
    "        # add f1 score\n",
    "        df['precision'] = precision\n",
    "        df['recall'] = recall\n",
    "        df['f1'] = f1_score\n",
    "\n",
    "        df.to_csv(f'./analysis_output/{file_name_without_extension}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "import os\n",
    "import glob\n",
    "\n",
    "all_files = glob.glob(\"./analysis_output/*.csv\")\n",
    "\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
    "pickles = glob.glob(\"./analysis_output/*.pickle\")\n",
    "\n",
    "# create a dictionary mapping basenames of pickle files to their full path\n",
    "pickle_map = {'.'.join(os.path.basename(pickle).rsplit('.', 2)[:-2]): pickle for pickle in pickles}\n",
    "\n",
    "def link_pickle_to_y_hat(y_hat_path):\n",
    "    ''' Extract the basename from the 'y_hat_path' and use it to find the corresponding pickle file '''\n",
    "    y_hat_basename = os.path.basename(y_hat_path)\n",
    "    y_hat_basename = '.'.join(y_hat_basename.rsplit('.', 2)[:-2])\n",
    "    return pickle_map.get(y_hat_basename, None)  # Returns None if no match is found\n",
    "\n",
    "# Apply the function to the 'y_hat_path' column to populate the 'pickle' column\n",
    "df['pickle'] = df['y_hat_path'].apply(link_pickle_to_y_hat)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude groups where all 'f1' values are NaN\n",
    "valid_groups = df.groupby('config_path')['f1'].transform('max').notna()\n",
    "df = df[valid_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'config_path' and get the index of the row with the max 'f1' score for each group\n",
    "idx = df.groupby(['config_path', 'mct_path'])['f1'].idxmax()\n",
    "\n",
    "# Sort the filtered DataFrame by 'recall' in descending order (highest first)\n",
    "best_df = df.sort_values(by='f1', ascending=False)\n",
    "\n",
    "# filter the best_df to keep only the best models for each config_path and scan\n",
    "best_df = best_df.groupby(['config_path', 'mct_path']).head(1)\n",
    "\n",
    "best_df.to_csv('best_models.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the filtered DataFrame\n",
    "print(best_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "this shows:\n",
    "- max_filter and center_of_mass are good in different circumstances\n",
    "- more orientations is better (>5) but let's do 3, 3, 3\n",
    "- avg_threshold is better smaller (0.2) for flammula but greater (0.5) for dampieri\n",
    "- peak_min_val is better smaller (0.2) for flammula by greater (0.5) for dampieri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Let's find the best parameters for inference for both dampieri and flammula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "The hyperiid rhabdom model appears to have a lot of false positives in predicted areas not around rhabdoms. This is liekly because the training data only included areas of scans around rhabdoms. So we can judge the performance of the model under these same circumstances, we need to only evaluate in areas of scans around rhabdoms. We do this below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "- Load paraphronima rhabdom inference\n",
    "- add filter to only evaluate area around ground truth labels\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Note, if evaluating the model in this way, we need to ensure readers know that detecting elongated features that are partially labelled (with single points) can be done by using our masking approach. However, the masking approach can lead to false positives in training data - by marking all high or low values around the point label, making the resulting model sensitive to false positives. It is therefore recommendeded to crop scans around the area of interest before running inference. In this case, combining masking with a cropped scan for inference, can provide great results on partially labelled data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
