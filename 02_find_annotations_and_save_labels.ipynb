{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Once you have organised all the files into your `raw_image_info.csv` file, we now need to generate the raw_label images to train our model. These will be calculated based off the raw_labels found in `raw_annotated_file_path`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# %gui qt5\n",
    "import torch\n",
    "import torchio as tio\n",
    "from torch.utils.data import DataLoader\n",
    "import scipy.io\n",
    "from scipy.ndimage import maximum_filter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import napari\n",
    "import h5py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generation of training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have volumes of micro-ct data that we are trying to label. However, the labels we currently have are 3d point locations, which isn't a format that our deep learning model can link spatially to our ct data. \n",
    "\n",
    "We want to convert these 3d point locations into a new \"prediction volume\". This is what we want our deep learning model to end up producing. It is also a format that our deep learning model can read, link spatially to our data and produce."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's load in the info we need from the csv file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "info = pd.read_csv('raw_image_info.csv')\n",
    "\n",
    "info"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                       image_file_path  \\\n",
       "0    //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...   \n",
       "1    //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...   \n",
       "2    //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...   \n",
       "3    //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...   \n",
       "4    //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...   \n",
       "..                                                 ...   \n",
       "151  //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...   \n",
       "152  //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...   \n",
       "153  //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...   \n",
       "154  //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...   \n",
       "155  //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...   \n",
       "\n",
       "                               raw_annotated_file_path  \\\n",
       "0    //home/jake/projects/mctv_resfiles/ants/diurna...   \n",
       "1    //home/jake/projects/mctv_resfiles/fiddlercrab...   \n",
       "2    //home/jake/projects/mctv_resfiles/fiddlercrab...   \n",
       "3    //home/jake/projects/mctv_resfiles/fiddlercrab...   \n",
       "4    //home/jake/projects/mctv_resfiles/fiddlercrab...   \n",
       "..                                                 ...   \n",
       "151                                                NaN   \n",
       "152                                                NaN   \n",
       "153                                                NaN   \n",
       "154                                                NaN   \n",
       "155                                                NaN   \n",
       "\n",
       "                                          check_status needs_transform_fix  \\\n",
       "0                                                 good               False   \n",
       "1                                                 good               False   \n",
       "2    needs regenerating with updated annotations fr...               False   \n",
       "3                                          mostly done               False   \n",
       "4                                                 good               False   \n",
       "..                                                 ...                 ...   \n",
       "151                                                NaN                 NaN   \n",
       "152                                                NaN                 NaN   \n",
       "153                                                NaN                 NaN   \n",
       "154                                                NaN                 NaN   \n",
       "155                                                NaN                 NaN   \n",
       "\n",
       "    needs_cropping  \n",
       "0            False  \n",
       "1            False  \n",
       "2            False  \n",
       "3             True  \n",
       "4            False  \n",
       "..             ...  \n",
       "151            NaN  \n",
       "152            NaN  \n",
       "153            NaN  \n",
       "154            NaN  \n",
       "155            NaN  \n",
       "\n",
       "[156 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file_path</th>\n",
       "      <th>raw_annotated_file_path</th>\n",
       "      <th>check_status</th>\n",
       "      <th>needs_transform_fix</th>\n",
       "      <th>needs_cropping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...</td>\n",
       "      <td>//home/jake/projects/mctv_resfiles/ants/diurna...</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...</td>\n",
       "      <td>//home/jake/projects/mctv_resfiles/fiddlercrab...</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...</td>\n",
       "      <td>//home/jake/projects/mctv_resfiles/fiddlercrab...</td>\n",
       "      <td>needs regenerating with updated annotations fr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...</td>\n",
       "      <td>//home/jake/projects/mctv_resfiles/fiddlercrab...</td>\n",
       "      <td>mostly done</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...</td>\n",
       "      <td>//home/jake/projects/mctv_resfiles/fiddlercrab...</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jak...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's view one of the label files to see where the data is"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "info.loc[0, 'raw_annotated_file_path']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'//home/jake/projects/mctv_resfiles/ants/diurnal_tarsata/tarsata.mat'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "f = h5py.File(info.loc[0, 'raw_annotated_file_path'], mode='r')\n",
    "\n",
    "f['save_dat'].visititems(lambda n, o:print(n, o))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ana <HDF5 group \"/save_dat/ana\" (1 members)>\n",
      "ana/para <HDF5 group \"/save_dat/ana/para\" (11 members)>\n",
      "ana/para/allow_splitting_rows_by_distance <HDF5 dataset \"allow_splitting_rows_by_distance\": shape (1, 1), type \"<f8\">\n",
      "ana/para/distance_for_pointtext <HDF5 dataset \"distance_for_pointtext\": shape (1, 1), type \"<f8\">\n",
      "ana/para/include_non_empty <HDF5 dataset \"include_non_empty\": shape (1, 1), type \"<f8\">\n",
      "ana/para/main_marker_size <HDF5 dataset \"main_marker_size\": shape (1, 1), type \"<f8\">\n",
      "ana/para/pt_th <HDF5 dataset \"pt_th\": shape (1, 1), type \"<f8\">\n",
      "ana/para/row_th <HDF5 dataset \"row_th\": shape (1, 1), type \"<f8\">\n",
      "ana/para/spline_stiff <HDF5 dataset \"spline_stiff\": shape (1, 1), type \"<f8\">\n",
      "ana/para/spline_stiff_smooth <HDF5 dataset \"spline_stiff_smooth\": shape (1, 1), type \"<f8\">\n",
      "ana/para/spline_stiff_soft_mult <HDF5 dataset \"spline_stiff_soft_mult\": shape (1, 1), type \"<f8\">\n",
      "ana/para/spline_stiff_z_mult <HDF5 dataset \"spline_stiff_z_mult\": shape (1, 1), type \"<f8\">\n",
      "ana/para/text_size <HDF5 dataset \"text_size\": shape (1, 1), type \"<f8\">\n",
      "autosave <HDF5 group \"/save_dat/autosave\" (4 members)>\n",
      "autosave/ctr <HDF5 dataset \"ctr\": shape (1, 1), type \"<f8\">\n",
      "autosave/filectr <HDF5 dataset \"filectr\": shape (1, 1), type \"<f8\">\n",
      "autosave/howmany <HDF5 dataset \"howmany\": shape (1, 1), type \"<f8\">\n",
      "autosave/howoften <HDF5 dataset \"howoften\": shape (1, 1), type \"<f8\">\n",
      "data <HDF5 group \"/save_dat/data\" (22 members)>\n",
      "data/colid <HDF5 dataset \"colid\": shape (2,), type \"<u8\">\n",
      "data/current_m <HDF5 dataset \"current_m\": shape (4, 4), type \"<f8\">\n",
      "data/current_pad <HDF5 dataset \"current_pad\": shape (3, 1), type \"<f8\">\n",
      "data/current_rotation <HDF5 dataset \"current_rotation\": shape (1, 1), type \"<f8\">\n",
      "data/current_trim <HDF5 dataset \"current_trim\": shape (3, 1), type \"<f8\">\n",
      "data/marked <HDF5 dataset \"marked\": shape (9, 5180), type \"<f8\">\n",
      "data/rowid <HDF5 dataset \"rowid\": shape (2,), type \"<u8\">\n",
      "data/search_x_weight <HDF5 dataset \"search_x_weight\": shape (1, 1), type \"<f8\">\n",
      "data/search_y_weight <HDF5 dataset \"search_y_weight\": shape (1, 1), type \"<f8\">\n",
      "data/search_z_weight <HDF5 dataset \"search_z_weight\": shape (1, 1), type \"<f8\">\n",
      "data/shell1 <HDF5 dataset \"shell1\": shape (1, 1), type \"<f8\">\n",
      "data/shell2 <HDF5 dataset \"shell2\": shape (1, 1), type \"<f8\">\n",
      "data/store_marked <HDF5 dataset \"store_marked\": shape (9, 5180), type \"<f8\">\n",
      "data/trans_r <HDF5 dataset \"trans_r\": shape (3, 6), type \"<f8\">\n",
      "data/undo <HDF5 group \"/save_dat/data/undo\" (3 members)>\n",
      "data/undo/dat <HDF5 group \"/save_dat/data/undo/dat\" (1 members)>\n",
      "data/undo/dat/dat <HDF5 dataset \"dat\": shape (50, 1), type \"|O\">\n",
      "data/undo/ind <HDF5 dataset \"ind\": shape (1, 1), type \"<f8\">\n",
      "data/undo/op <HDF5 dataset \"op\": shape (50, 1), type \"|O\">\n",
      "data/update_row_col1 <HDF5 dataset \"update_row_col1\": shape (1, 1), type \"<f8\">\n",
      "data/update_row_col2 <HDF5 dataset \"update_row_col2\": shape (1, 1), type \"<f8\">\n",
      "data/window_current_datam <HDF5 dataset \"window_current_datam\": shape (1, 1), type \"<f8\">\n",
      "data/window_current_image_rotation <HDF5 dataset \"window_current_image_rotation\": shape (3, 1), type \"<f8\">\n",
      "data/window_current_inv_datam <HDF5 dataset \"window_current_inv_datam\": shape (1, 1), type \"<f8\">\n",
      "data/window_current_m <HDF5 dataset \"window_current_m\": shape (1, 1), type \"<f8\">\n",
      "data/window_last_inverse <HDF5 dataset \"window_last_inverse\": shape (1, 1), type \"<f8\">\n",
      "figh <HDF5 dataset \"figh\": shape (1, 1), type \"<f8\">\n",
      "figh_ana <HDF5 dataset \"figh_ana\": shape (1, 1), type \"<f8\">\n",
      "figh_fana <HDF5 dataset \"figh_fana\": shape (1, 1), type \"<f8\">\n",
      "gd <HDF5 group \"/save_dat/gd\" (23 members)>\n",
      "gd/ax4_zoom <HDF5 dataset \"ax4_zoom\": shape (1, 1), type \"<f8\">\n",
      "gd/axorder <HDF5 dataset \"axorder\": shape (4, 1), type \"<f8\">\n",
      "gd/cm_highlim <HDF5 dataset \"cm_highlim\": shape (1, 1), type \"<f8\">\n",
      "gd/cm_lowlim <HDF5 dataset \"cm_lowlim\": shape (1, 1), type \"<f8\">\n",
      "gd/cols <HDF5 dataset \"cols\": shape (3, 3), type \"<f8\">\n",
      "gd/depthsel <HDF5 dataset \"depthsel\": shape (2, 1), type \"<f8\">\n",
      "gd/imf <HDF5 dataset \"imf\": shape (1, 1), type \"<f8\">\n",
      "gd/in_row_pt_dist <HDF5 dataset \"in_row_pt_dist\": shape (1, 1), type \"<f8\">\n",
      "gd/markerfontsize <HDF5 dataset \"markerfontsize\": shape (1, 1), type \"<f8\">\n",
      "gd/markersize <HDF5 dataset \"markersize\": shape (1, 1), type \"<f8\">\n",
      "gd/out_in_val <HDF5 dataset \"out_in_val\": shape (1, 1), type \"<f8\">\n",
      "gd/plot_limit_text_numbers <HDF5 dataset \"plot_limit_text_numbers\": shape (1, 1), type \"<f8\">\n",
      "gd/plot_lines <HDF5 dataset \"plot_lines\": shape (1, 1), type \"<f8\">\n",
      "gd/plot_text_c <HDF5 dataset \"plot_text_c\": shape (1, 1), type \"|u1\">\n",
      "gd/plot_text_h <HDF5 dataset \"plot_text_h\": shape (1, 1), type \"|u1\">\n",
      "gd/point_dist_expected <HDF5 dataset \"point_dist_expected\": shape (1, 1), type \"<f8\">\n",
      "gd/pressed_col <HDF5 dataset \"pressed_col\": shape (3, 1), type \"<f8\">\n",
      "gd/rotate_orientation <HDF5 dataset \"rotate_orientation\": shape (14, 1), type \"<u2\">\n",
      "gd/select_dist <HDF5 dataset \"select_dist\": shape (1, 1), type \"<f8\">\n",
      "gd/sout_in <HDF5 dataset \"sout_in\": shape (1, 1), type \"<f8\">\n",
      "gd/trans <HDF5 group \"/save_dat/gd/trans\" (3 members)>\n",
      "gd/trans/l <HDF5 dataset \"l\": shape (6, 1), type \"|O\">\n",
      "gd/trans/r <HDF5 dataset \"r\": shape (6, 1), type \"|O\">\n",
      "gd/trans/ra <HDF5 dataset \"ra\": shape (6, 1), type \"|O\">\n",
      "gd/unpressed_col <HDF5 dataset \"unpressed_col\": shape (3, 1), type \"<f8\">\n",
      "gd/use_altfigure <HDF5 dataset \"use_altfigure\": shape (4, 1), type \"<f8\">\n",
      "rmfields <HDF5 group \"/save_dat/rmfields\" (2 members)>\n",
      "rmfields/s <HDF5 dataset \"s\": shape (4, 1), type \"|O\">\n",
      "rmfields/stack <HDF5 dataset \"stack\": shape (5, 1), type \"|O\">\n",
      "rotation_size_limit <HDF5 dataset \"rotation_size_limit\": shape (1, 1), type \"<f8\">\n",
      "rotation_size_min <HDF5 dataset \"rotation_size_min\": shape (1, 1), type \"<f8\">\n",
      "savename <HDF5 dataset \"savename\": shape (32, 1), type \"<u2\">\n",
      "stack <HDF5 group \"/save_dat/stack\" (24 members)>\n",
      "stack/autobackward <HDF5 dataset \"autobackward\": shape (1, 1), type \"<f8\">\n",
      "stack/autocentering <HDF5 dataset \"autocentering\": shape (1, 1), type \"|u1\">\n",
      "stack/autocentering_all <HDF5 dataset \"autocentering_all\": shape (1, 1), type \"|u1\">\n",
      "stack/autoforward <HDF5 dataset \"autoforward\": shape (1, 1), type \"<f8\">\n",
      "stack/cl <HDF5 dataset \"cl\": shape (3, 1), type \"<f8\">\n",
      "stack/crossh <HDF5 group \"/save_dat/stack/crossh\" (9 members)>\n",
      "stack/crossh/im1_1 <HDF5 dataset \"im1_1\": shape (1, 6), type \"<u4\">\n",
      "stack/crossh/im1_2 <HDF5 dataset \"im1_2\": shape (1, 6), type \"<u4\">\n",
      "stack/crossh/im2_1 <HDF5 dataset \"im2_1\": shape (1, 6), type \"<u4\">\n",
      "stack/crossh/im2_2 <HDF5 dataset \"im2_2\": shape (1, 6), type \"<u4\">\n",
      "stack/crossh/im3_1 <HDF5 dataset \"im3_1\": shape (1, 6), type \"<u4\">\n",
      "stack/crossh/im3_2 <HDF5 dataset \"im3_2\": shape (1, 6), type \"<u4\">\n",
      "stack/crossh/im4_1 <HDF5 dataset \"im4_1\": shape (1, 6), type \"<u4\">\n",
      "stack/crossh/im4_2 <HDF5 dataset \"im4_2\": shape (1, 6), type \"<u4\">\n",
      "stack/crossh/im4_3 <HDF5 dataset \"im4_3\": shape (1, 6), type \"<u4\">\n",
      "stack/current_rotation_string <HDF5 dataset \"current_rotation_string\": shape (2,), type \"<u8\">\n",
      "stack/datasavename <HDF5 dataset \"datasavename\": shape (2,), type \"<u8\">\n",
      "stack/fixedzoom_level <HDF5 dataset \"fixedzoom_level\": shape (1, 1), type \"<f8\">\n",
      "stack/image_format <HDF5 dataset \"image_format\": shape (5, 1), type \"<u2\">\n",
      "stack/local_interactive <HDF5 dataset \"local_interactive\": shape (1, 1), type \"<f8\">\n",
      "stack/origin <HDF5 dataset \"origin\": shape (3, 1), type \"<f8\">\n",
      "stack/rimage1 <HDF5 dataset \"rimage1\": shape (251, 251), type \"<f4\">\n",
      "stack/rimage2 <HDF5 dataset \"rimage2\": shape (251, 251), type \"<f4\">\n",
      "stack/rimage3 <HDF5 dataset \"rimage3\": shape (251, 251), type \"<f4\">\n",
      "stack/rimage_sel1 <HDF5 dataset \"rimage_sel1\": shape (251, 1), type \"<f8\">\n",
      "stack/rimage_sel2 <HDF5 dataset \"rimage_sel2\": shape (251, 1), type \"<f8\">\n",
      "stack/rimage_sel3 <HDF5 dataset \"rimage_sel3\": shape (251, 1), type \"<f8\">\n",
      "stack/rotate_interactive <HDF5 dataset \"rotate_interactive\": shape (1, 1), type \"<f8\">\n",
      "stack/rotated <HDF5 dataset \"rotated\": shape (1, 1), type \"<f8\">\n",
      "stack/savestore_cl <HDF5 dataset \"savestore_cl\": shape (3, 1), type \"<f8\">\n",
      "stack/show_all_directions <HDF5 dataset \"show_all_directions\": shape (1, 1), type \"<f8\">\n",
      "stack/start_with <HDF5 dataset \"start_with\": shape (6, 1), type \"<u2\">\n",
      "stack/view_interactive <HDF5 dataset \"view_interactive\": shape (1, 1), type \"<f8\">\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "pd.DataFrame(np.array(f['save_dat/data/marked']))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         0           1           2           3           4           5     \\\n",
       "0  210.378869  208.715220  203.161893  201.773182  216.326074  211.377609   \n",
       "1  610.125062  608.291494  617.583415  627.088487  597.115077  601.899662   \n",
       "2  618.403081  605.820510  597.841257  598.722290  622.821452  613.526117   \n",
       "3   11.000000   11.000000   11.000000   11.000000   12.000000   12.000000   \n",
       "4   52.000000   53.000000   54.000000   55.000000   51.000000   52.000000   \n",
       "5    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "6    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "7    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "8    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "         6           7           8           9     ...        5170  \\\n",
       "0  207.015189  202.492341  200.279329  199.304796  ...  745.026416   \n",
       "1  603.774637  614.244095  624.459194  633.980652  ...  443.376608   \n",
       "2  597.957832  588.549029  587.718511  594.389523  ...  417.117119   \n",
       "3   12.000000   12.000000   12.000000   12.000000  ...   76.000000   \n",
       "4   53.000000   54.000000   55.000000   56.000000  ...   42.000000   \n",
       "5    0.000000    0.000000    0.000000    0.000000  ...    1.000000   \n",
       "6    0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "7    0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "8    0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "\n",
       "         5171        5172        5173        5174        5175        5176  \\\n",
       "0  746.956193  749.140737  752.587520  755.880528  750.993639  750.950449   \n",
       "1  430.567993  420.227072  411.458105  403.947978  401.989690  397.802645   \n",
       "2  429.199554  438.165841  448.930036  460.621231  462.513364  472.234956   \n",
       "3   76.000000   76.000000   76.000000   76.000000   76.000000   76.000000   \n",
       "4   43.000000   44.000000   45.000000   46.000000   47.000000   48.000000   \n",
       "5    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "6    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "7    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "8    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "         5177        5178        5179  \n",
       "0  748.290773  750.992589  752.272633  \n",
       "1  439.233097  430.750563  421.386247  \n",
       "2  424.412174  436.990685  446.607978  \n",
       "3   77.000000   77.000000   77.000000  \n",
       "4   43.000000   44.000000   45.000000  \n",
       "5    1.000000    1.000000    1.000000  \n",
       "6    0.000000    0.000000    0.000000  \n",
       "7    0.000000    0.000000    0.000000  \n",
       "8    0.000000    0.000000    0.000000  \n",
       "\n",
       "[9 rows x 5180 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5170</th>\n",
       "      <th>5171</th>\n",
       "      <th>5172</th>\n",
       "      <th>5173</th>\n",
       "      <th>5174</th>\n",
       "      <th>5175</th>\n",
       "      <th>5176</th>\n",
       "      <th>5177</th>\n",
       "      <th>5178</th>\n",
       "      <th>5179</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210.378869</td>\n",
       "      <td>208.715220</td>\n",
       "      <td>203.161893</td>\n",
       "      <td>201.773182</td>\n",
       "      <td>216.326074</td>\n",
       "      <td>211.377609</td>\n",
       "      <td>207.015189</td>\n",
       "      <td>202.492341</td>\n",
       "      <td>200.279329</td>\n",
       "      <td>199.304796</td>\n",
       "      <td>...</td>\n",
       "      <td>745.026416</td>\n",
       "      <td>746.956193</td>\n",
       "      <td>749.140737</td>\n",
       "      <td>752.587520</td>\n",
       "      <td>755.880528</td>\n",
       "      <td>750.993639</td>\n",
       "      <td>750.950449</td>\n",
       "      <td>748.290773</td>\n",
       "      <td>750.992589</td>\n",
       "      <td>752.272633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>610.125062</td>\n",
       "      <td>608.291494</td>\n",
       "      <td>617.583415</td>\n",
       "      <td>627.088487</td>\n",
       "      <td>597.115077</td>\n",
       "      <td>601.899662</td>\n",
       "      <td>603.774637</td>\n",
       "      <td>614.244095</td>\n",
       "      <td>624.459194</td>\n",
       "      <td>633.980652</td>\n",
       "      <td>...</td>\n",
       "      <td>443.376608</td>\n",
       "      <td>430.567993</td>\n",
       "      <td>420.227072</td>\n",
       "      <td>411.458105</td>\n",
       "      <td>403.947978</td>\n",
       "      <td>401.989690</td>\n",
       "      <td>397.802645</td>\n",
       "      <td>439.233097</td>\n",
       "      <td>430.750563</td>\n",
       "      <td>421.386247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>618.403081</td>\n",
       "      <td>605.820510</td>\n",
       "      <td>597.841257</td>\n",
       "      <td>598.722290</td>\n",
       "      <td>622.821452</td>\n",
       "      <td>613.526117</td>\n",
       "      <td>597.957832</td>\n",
       "      <td>588.549029</td>\n",
       "      <td>587.718511</td>\n",
       "      <td>594.389523</td>\n",
       "      <td>...</td>\n",
       "      <td>417.117119</td>\n",
       "      <td>429.199554</td>\n",
       "      <td>438.165841</td>\n",
       "      <td>448.930036</td>\n",
       "      <td>460.621231</td>\n",
       "      <td>462.513364</td>\n",
       "      <td>472.234956</td>\n",
       "      <td>424.412174</td>\n",
       "      <td>436.990685</td>\n",
       "      <td>446.607978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 5180 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/1891607671.py:1: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'dicom'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's do the same thing with scipy in the case where the matlab file is an old type"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "dir = info.loc[2, 'raw_annotated_file_path']\n",
    "\n",
    "mat = scipy.io.loadmat(dir)\n",
    "\n",
    "file_type = mat['save_dat'][0]['stack'][0]['image_format'][0][0][0]\n",
    "classification = pd.DataFrame(mat['save_dat'][0]['data'][0][0][0][0]).iloc[:,5]\n",
    "points = pd.DataFrame(mat['save_dat'][0]['data'][0][0][0][0]).iloc[:,[2, 1, 0]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "file_type"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'nifti'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "classification"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "13914    0.0\n",
       "13915    0.0\n",
       "13916    0.0\n",
       "13917    0.0\n",
       "13918    0.0\n",
       "Name: 5, Length: 13919, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "points"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                2           1           0\n",
       "0      492.100525  456.952484  814.715515\n",
       "1      480.170630  447.510213  815.730097\n",
       "2      503.717560  467.951080  814.715515\n",
       "3      502.127380  481.467499  814.715515\n",
       "4      488.699310  472.278168  817.098267\n",
       "...           ...         ...         ...\n",
       "13914  222.343597  455.246357  263.753937\n",
       "13915  234.833111  454.485577  257.645381\n",
       "13916  284.391574  439.530726  803.305356\n",
       "13917  397.746509  222.228610  389.085456\n",
       "13918  240.247288  740.072203  440.675276\n",
       "\n",
       "[13919 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492.100525</td>\n",
       "      <td>456.952484</td>\n",
       "      <td>814.715515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>480.170630</td>\n",
       "      <td>447.510213</td>\n",
       "      <td>815.730097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>503.717560</td>\n",
       "      <td>467.951080</td>\n",
       "      <td>814.715515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>502.127380</td>\n",
       "      <td>481.467499</td>\n",
       "      <td>814.715515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>488.699310</td>\n",
       "      <td>472.278168</td>\n",
       "      <td>817.098267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13914</th>\n",
       "      <td>222.343597</td>\n",
       "      <td>455.246357</td>\n",
       "      <td>263.753937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13915</th>\n",
       "      <td>234.833111</td>\n",
       "      <td>454.485577</td>\n",
       "      <td>257.645381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13916</th>\n",
       "      <td>284.391574</td>\n",
       "      <td>439.530726</td>\n",
       "      <td>803.305356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13917</th>\n",
       "      <td>397.746509</td>\n",
       "      <td>222.228610</td>\n",
       "      <td>389.085456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13918</th>\n",
       "      <td>240.247288</td>\n",
       "      <td>740.072203</td>\n",
       "      <td>440.675276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13919 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's the locations of the annotations in x, y and z space"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "pd.DataFrame(np.array(f['save_dat']['data']['marked'])).iloc[[2, 1, 0], :]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         0           1           2           3           4           5     \\\n",
       "2  618.403081  605.820510  597.841257  598.722290  622.821452  613.526117   \n",
       "1  610.125062  608.291494  617.583415  627.088487  597.115077  601.899662   \n",
       "0  210.378869  208.715220  203.161893  201.773182  216.326074  211.377609   \n",
       "\n",
       "         6           7           8           9     ...        5170  \\\n",
       "2  597.957832  588.549029  587.718511  594.389523  ...  417.117119   \n",
       "1  603.774637  614.244095  624.459194  633.980652  ...  443.376608   \n",
       "0  207.015189  202.492341  200.279329  199.304796  ...  745.026416   \n",
       "\n",
       "         5171        5172        5173        5174        5175        5176  \\\n",
       "2  429.199554  438.165841  448.930036  460.621231  462.513364  472.234956   \n",
       "1  430.567993  420.227072  411.458105  403.947978  401.989690  397.802645   \n",
       "0  746.956193  749.140737  752.587520  755.880528  750.993639  750.950449   \n",
       "\n",
       "         5177        5178        5179  \n",
       "2  424.412174  436.990685  446.607978  \n",
       "1  439.233097  430.750563  421.386247  \n",
       "0  748.290773  750.992589  752.272633  \n",
       "\n",
       "[3 rows x 5180 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5170</th>\n",
       "      <th>5171</th>\n",
       "      <th>5172</th>\n",
       "      <th>5173</th>\n",
       "      <th>5174</th>\n",
       "      <th>5175</th>\n",
       "      <th>5176</th>\n",
       "      <th>5177</th>\n",
       "      <th>5178</th>\n",
       "      <th>5179</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>618.403081</td>\n",
       "      <td>605.820510</td>\n",
       "      <td>597.841257</td>\n",
       "      <td>598.722290</td>\n",
       "      <td>622.821452</td>\n",
       "      <td>613.526117</td>\n",
       "      <td>597.957832</td>\n",
       "      <td>588.549029</td>\n",
       "      <td>587.718511</td>\n",
       "      <td>594.389523</td>\n",
       "      <td>...</td>\n",
       "      <td>417.117119</td>\n",
       "      <td>429.199554</td>\n",
       "      <td>438.165841</td>\n",
       "      <td>448.930036</td>\n",
       "      <td>460.621231</td>\n",
       "      <td>462.513364</td>\n",
       "      <td>472.234956</td>\n",
       "      <td>424.412174</td>\n",
       "      <td>436.990685</td>\n",
       "      <td>446.607978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>610.125062</td>\n",
       "      <td>608.291494</td>\n",
       "      <td>617.583415</td>\n",
       "      <td>627.088487</td>\n",
       "      <td>597.115077</td>\n",
       "      <td>601.899662</td>\n",
       "      <td>603.774637</td>\n",
       "      <td>614.244095</td>\n",
       "      <td>624.459194</td>\n",
       "      <td>633.980652</td>\n",
       "      <td>...</td>\n",
       "      <td>443.376608</td>\n",
       "      <td>430.567993</td>\n",
       "      <td>420.227072</td>\n",
       "      <td>411.458105</td>\n",
       "      <td>403.947978</td>\n",
       "      <td>401.989690</td>\n",
       "      <td>397.802645</td>\n",
       "      <td>439.233097</td>\n",
       "      <td>430.750563</td>\n",
       "      <td>421.386247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210.378869</td>\n",
       "      <td>208.715220</td>\n",
       "      <td>203.161893</td>\n",
       "      <td>201.773182</td>\n",
       "      <td>216.326074</td>\n",
       "      <td>211.377609</td>\n",
       "      <td>207.015189</td>\n",
       "      <td>202.492341</td>\n",
       "      <td>200.279329</td>\n",
       "      <td>199.304796</td>\n",
       "      <td>...</td>\n",
       "      <td>745.026416</td>\n",
       "      <td>746.956193</td>\n",
       "      <td>749.140737</td>\n",
       "      <td>752.587520</td>\n",
       "      <td>755.880528</td>\n",
       "      <td>750.993639</td>\n",
       "      <td>750.950449</td>\n",
       "      <td>748.290773</td>\n",
       "      <td>750.992589</td>\n",
       "      <td>752.272633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 5180 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's the category associated with the annotations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "pd.DataFrame(np.array(f['save_dat']['data']['marked'])).iloc[5, :]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "5175    1.0\n",
       "5176    1.0\n",
       "5177    1.0\n",
       "5178    1.0\n",
       "5179    1.0\n",
       "Name: 5, Length: 5180, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's set up some helper functions to generate annotated prediction volumes for our analysis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def _bool_3d_filter(array, indices, buffer=3):\n",
    "    # TODO: do i need to subtract 1 to these because of differences in matlab and python indexing?\n",
    "    for i in range(len(indices[0])):\n",
    "        ind = (indices[0][i], indices[1][i], indices[2][i])\n",
    "        x_min = max(ind[0] - buffer, 0)\n",
    "        x_max = min(ind[0] + buffer, array.shape[1])\n",
    "        y_min = max(ind[1] - buffer, 0)\n",
    "        y_max = min(ind[1] + buffer, array.shape[2])\n",
    "        z_min = max(ind[2] - buffer, 0)\n",
    "        z_max = min(ind[2] + buffer, array.shape[3])\n",
    "        \n",
    "        array[0, x_min:x_max, y_min:y_max, z_min:z_max] = 1\n",
    "    \n",
    "    return array\n",
    "\n",
    "def _load_point_data(dir, swap_xy):\n",
    "    print('loading point data from .mat files...')\n",
    "    # load classifications\n",
    "    if h5py.is_hdf5(dir):\n",
    "        f = h5py.File(dir, mode='r')\n",
    "        classification = pd.DataFrame(np.array(f['save_dat']['data']['marked'])).iloc[5, :]\n",
    "        file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n",
    "        print(f'Detected {file_type} file type')\n",
    "        if swap_xy:\n",
    "            points = pd.DataFrame(np.array(f['save_dat']['data']['marked'])).iloc[[2, 0, 1], :].T\n",
    "        else:\n",
    "            points = pd.DataFrame(np.array(f['save_dat']['data']['marked'])).iloc[[2, 1, 0], :].T\n",
    "    else:\n",
    "        mat = scipy.io.loadmat(dir)\n",
    "        classification = pd.DataFrame(mat['save_dat'][0]['data'][0][0][0][0]).iloc[:,5]\n",
    "        file_type = mat['save_dat'][0]['stack'][0]['image_format'][0][0][0]\n",
    "        print(f'Detected {file_type} file type')\n",
    "        if swap_xy:\n",
    "            points = pd.DataFrame(mat['save_dat'][0]['data'][0][0][0][0]).iloc[:,[2, 0, 1]]\n",
    "        else:\n",
    "            points = pd.DataFrame(mat['save_dat'][0]['data'][0][0][0][0]).iloc[:,[2, 1, 0]]\n",
    "    \n",
    "    points.columns = ['x', 'y', 'z']\n",
    "\n",
    "\n",
    "    # convert back to numpy array and round to nearest voxel\n",
    "    points = np.array(points)\n",
    "    points = np.round(points).astype(int)\n",
    "\n",
    "    # get corneas and rhabdom locations with x, y and z data\n",
    "    cornea_indx = (classification == 0) | (classification == 2)\n",
    "    rhabdom_indx = (classification == 1) | (classification == 3)\n",
    "    cornea_locations = points[cornea_indx, :]\n",
    "    rhabdom_locations = points[rhabdom_indx, :]\n",
    "\n",
    "    return cornea_locations, rhabdom_locations\n",
    "\n",
    "def _point_to_segmentation_vol(image, cornea_locations, rhabdom_locations, flipY):\n",
    "    print('converting point data to segmentation volume...')\n",
    "    # create empty matrix the size of original data\n",
    "    print('creating an empty image')\n",
    "    empty = image.copy().astype('bool_')\n",
    "    empty[:, :, :] = 0\n",
    "    \n",
    "    print('copying empty images')\n",
    "    corneas = empty.copy()\n",
    "    rhabdoms = empty.copy()\n",
    "    \n",
    "#     print('assigning positions of corneas and rhabdoms')\n",
    "#     corneas[\n",
    "#         0,\n",
    "#         cornea_locations[:, 2],\n",
    "#         cornea_locations[:, 1],\n",
    "#         cornea_locations[:, 0]\n",
    "#     ] = 1\n",
    "#     rhabdoms[\n",
    "#         0,\n",
    "#         rhabdom_locations[:, 2],\n",
    "#         rhabdom_locations[:, 1],\n",
    "#         rhabdom_locations[:, 0]\n",
    "#     ] = 1\n",
    "    \n",
    "#     # now use a maximum filter to make points a slightly larger areak\n",
    "#     # note, that maximum filter makes predictions a cube without rounded edges\n",
    "#     # a gaussian filter may be more appropriate\n",
    "#     print('running maximum filter')\n",
    "#     corneas = maximum_filter(corneas, size=3)\n",
    "#     rhabdoms = maximum_filter(rhabdoms, size=3)\n",
    "    \n",
    "    print('assigning positions of corneas and rhabdoms with buffer')\n",
    "    corneas = _bool_3d_filter(\n",
    "        corneas,\n",
    "        (\n",
    "            cornea_locations[:, 2],\n",
    "            cornea_locations[:, 1],\n",
    "            cornea_locations[:, 0]\n",
    "        ),\n",
    "        buffer=1\n",
    "    )\n",
    "    \n",
    "    rhabdoms = _bool_3d_filter(\n",
    "        rhabdoms,\n",
    "        (\n",
    "            rhabdom_locations[:, 2],\n",
    "            rhabdom_locations[:, 1],\n",
    "            rhabdom_locations[:, 0]\n",
    "        ),\n",
    "        buffer=1\n",
    "    )\n",
    "    \n",
    "    # now merge both into a single prediction volume\n",
    "    # 0 = nothing\n",
    "    # 1 = cornea\n",
    "    # 2 = rhabdom\n",
    "    print('merging cornea and rhabdom images into single volume')\n",
    "\n",
    "    prediction = empty.astype(np.int16)\n",
    "    prediction[corneas] = 1\n",
    "    prediction[rhabdoms] = 2\n",
    "\n",
    "    if flipY:\n",
    "        print('Flipping Y because file was annotated with dicom')\n",
    "        prediction = np.flip(prediction, 2).copy()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def create_annotated_volumes(dir, image, swap_xy, flip_y):\n",
    "    cornea_locations, rhabdom_locations = _load_point_data(dir, swap_xy)\n",
    "    annotated_vol = _point_to_segmentation_vol(\n",
    "        image,\n",
    "        cornea_locations,\n",
    "        rhabdom_locations,\n",
    "        flip_y\n",
    "    )\n",
    "    print('done.')\n",
    "    return annotated_vol\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "n_rows = info.shape[0]\n",
    "\n",
    "out_label_dir = '//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/'\n",
    "\n",
    "plot = False\n",
    "\n",
    "for i in range(n_rows):\n",
    "    img = info.loc[i, 'image_file_path']\n",
    "    label = info.loc[i, 'raw_annotated_file_path']\n",
    "    swap_xy = info.loc[i, 'swap_xy']\n",
    "    flip_y = info.loc[i, 'flip_y']\n",
    "\n",
    "    p = Path(img)\n",
    "    filename = p.stem\n",
    "    out_path = out_label_dir + filename + '.nii'\n",
    "    \n",
    "    if not os.path.isfile(out_path):\n",
    "        print('starting conversion of ' + filename)\n",
    "        transform = tio.ToCanonical()\n",
    "        img = tio.ScalarImage(\n",
    "            img\n",
    "        )\n",
    "        ann = tio.LabelMap(\n",
    "            tensor=create_annotated_volumes(\n",
    "                label,\n",
    "                img.data.numpy(),\n",
    "                swap_xy,\n",
    "                flip_y\n",
    "            ),\n",
    "            affine=img.affine\n",
    "        )\n",
    "        \n",
    "        \n",
    "        if plot:\n",
    "            viewer = napari.Viewer()\n",
    "            viewer.dims.ndisplay = 3 # toggle 3 dimensional view\n",
    "            viewer.add_image(img.data.numpy())\n",
    "            viewer.add_image(ann.data.numpy())\n",
    "\n",
    "        # now save the label/annotation\n",
    "        print('saving to ' + out_path)\n",
    "        ann.save(out_path)\n",
    "    else:\n",
    "        print(out_path + ' has already been created, so skipping')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Needs transform fix: False\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/diurnal_tarsata.nii has already been created, so skipping\n",
      "Needs transform fix: False\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/dampieri_20151218.nii has already been created, so skipping\n",
      "Needs transform fix: False\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/dampieri_20200218_male_left_1676.nii has already been created, so skipping\n",
      "Needs transform fix: False\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/dampieri_male_16.nii has already been created, so skipping\n",
      "Needs transform fix: False\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/flammula_20180307.nii has already been created, so skipping\n",
      "Needs transform fix: False\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/flammula_20190925_male_left.nii has already been created, so skipping\n",
      "Needs transform fix: False\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/flammula_20200327_female_left_178_fullres_cropped.nii has already been created, so skipping\n",
      "Needs transform fix: False\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/neohelice_20190616_1_cropped.nii has already been created, so skipping\n",
      "Needs transform fix: False\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/Hyperia_02.nii has already been created, so skipping\n",
      "Needs transform fix: True\n",
      "starting conversion of Hyperia_01_head_FEG190604_014B\n",
      "loading point data from .mat files...\n",
      "Detected nifti file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n",
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/Hyperia_01_head_FEG190604_014B.nii\n",
      "Needs transform fix: True\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/Leptocotis_02_head_FEG190214_005b.nii has already been created, so skipping\n",
      "Needs transform fix: True\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/P_crassipes_FEG190213_003b_02_head.nii has already been created, so skipping\n",
      "Needs transform fix: True\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/P_crassipes_FEG190801_034_head.nii has already been created, so skipping\n",
      "Needs transform fix: True\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/P_crassipes_FEG191022_077A_highpriority.nii has already been created, so skipping\n",
      "Needs transform fix: True\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/P_crassipes_FEG191022_077B_highpriority.nii has already been created, so skipping\n",
      "Needs transform fix: True\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/P_crassipes_FEG200129_099_highpriority.nii has already been created, so skipping\n",
      "Needs transform fix: True\n",
      "//mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/P_crassipes_FEG200205_108.nii has already been created, so skipping\n",
      "Needs transform fix: True\n",
      "starting conversion of P_gracilis_FEG190213_003a_head_HIGHESTpriority\n",
      "loading point data from .mat files...\n",
      "Detected auto file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/P_gracilis_FEG190213_003a_head_HIGHESTpriority.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of P_robisoni_FEG200205_105-head\n",
      "loading point data from .mat files...\n",
      "Detected auto file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/P_robisoni_FEG200205_105-head.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of Paraphronima_FEG181024_03_head_sp3_1423158b\n",
      "loading point data from .mat files...\n",
      "Detected auto file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/Paraphronima_FEG181024_03_head_sp3_1423158b.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of Paraphronima_FEG181024_head_sp_3_1423158a\n",
      "loading point data from .mat files...\n",
      "Detected auto file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/Paraphronima_FEG181024_head_sp_3_1423158a.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of Paraphronima_FEG181024_head_sp_8_1423158\n",
      "loading point data from .mat files...\n",
      "Detected auto file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/Paraphronima_FEG181024_head_sp_8_1423158.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of Paraphronima_FEG191028_078\n",
      "loading point data from .mat files...\n",
      "Detected dicom file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/Paraphronima_FEG191028_078.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of Paraphronima_head_04_FEG200130_103\n",
      "loading point data from .mat files...\n",
      "Detected auto file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/Paraphronima_head_04_FEG200130_103.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of Paraphronima_head_05_FEG200130_102\n",
      "loading point data from .mat files...\n",
      "Detected auto file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/Paraphronima_head_05_FEG200130_102.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of Phronima_04_FEG200107_089\n",
      "loading point data from .mat files...\n",
      "Detected nifti file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n",
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/Phronima_04_FEG200107_089.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of Phronima_05_FEG200107_090\n",
      "loading point data from .mat files...\n",
      "Detected nifti file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n",
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/Phronima_05_FEG200107_090.nii\n",
      "Needs transform fix: False\n",
      "starting conversion of phronima_20180403\n",
      "loading point data from .mat files...\n",
      "Detected nifti file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n",
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/phronima_20180403.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of phronima_20190608\n",
      "loading point data from .mat files...\n",
      "Detected dicom file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/phronima_20190608.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of Platyscelus_02_FEG191112_082\n",
      "loading point data from .mat files...\n",
      "Detected nifti file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n",
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/Platyscelus_02_FEG191112_082.nii\n",
      "Needs transform fix: False\n",
      "starting conversion of psyllid_20190906_female_eye\n",
      "loading point data from .mat files...\n",
      "Detected nifti file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n",
      "copying empty images\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/psyllid_20190906_female_eye.nii\n",
      "Needs transform fix: False\n",
      "starting conversion of psyllid_20190906_male_eye\n",
      "loading point data from .mat files...\n",
      "Detected dicom file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n",
      "copying empty images\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/psyllid_20190906_male_eye.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of flammula_male_23_6_left\n",
      "loading point data from .mat files...\n",
      "Detected dicom file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n",
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/flammula_male_23_6_left.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of brachyscelus_01\n",
      "loading point data from .mat files...\n",
      "Detected nifti file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n",
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/brachyscelus_01.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of platyscelus_01_downsampled\n",
      "loading point data from .mat files...\n",
      "Detected nifti file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n",
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/platyscelus_01_downsampled.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of streetsia_20180409\n",
      "loading point data from .mat files...\n",
      "Detected nifti file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n",
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/streetsia_20180409.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of streetsia_20190308\n",
      "loading point data from .mat files...\n",
      "Detected nifti file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/streetsia_20190308.nii\n",
      "Needs transform fix: True\n",
      "starting conversion of streetsia_20190325\n",
      "loading point data from .mat files...\n",
      "Detected dicom file type\n",
      "converting point data to segmentation volume...\n",
      "creating an empty image\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_24076/2796752783.py:22: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  file_type = f['save_dat']['stack']['image_format'].value.tobytes()[::2].decode()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "copying empty images\n",
      "assigning positions of corneas and rhabdoms with buffer\n",
      "merging cornea and rhabdom images into single volume\n",
      "Flipping Y because file was annotated with dicom\n",
      "done.\n",
      "saving to //mnt/d37c99c5-3b94-47b9-9965-c66fd9a16e23/jake/mctnet_data/raw_labels/streetsia_20190325.nii\n",
      "Needs transform fix: nan\n",
      "starting conversion of 20181221L_female_cropped\n",
      "loading point data from .mat files...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not float",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24076/1530128770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         )\n\u001b[1;32m     26\u001b[0m         ann = tio.LabelMap(\n\u001b[0;32m---> 27\u001b[0;31m             tensor=create_annotated_volumes(\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24076/2796752783.py\u001b[0m in \u001b[0;36mcreate_annotated_volumes\u001b[0;34m(dir, image, transform)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_annotated_volumes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mcornea_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhabdom_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_point_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     annotated_vol = _point_to_segmentation_vol(\n\u001b[1;32m    125\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24076/2796752783.py\u001b[0m in \u001b[0;36m_load_point_data\u001b[0;34m(dir, transform)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading point data from .mat files...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# load classifications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save_dat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'marked'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/mctnet/venv/lib/python3.8/site-packages/h5py/_hl/base.py\u001b[0m in \u001b[0;36mis_hdf5\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\" Determine if a file is valid HDF5 (False if it doesn't exist). \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not float"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, that we have our raw labels and images, let's move to step 03"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f726fa9e3d96265230c4503e543f5c3f73ae5ded033c480caee9e2a2a13a4b04"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}